{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 14 Reinforcement Learning in der Sprachtechnologie\n",
    "## QA, Discussions, Feedback & Philosphy\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Announcements\n",
    "\n",
    "\n",
    "#### Homework\n",
    "        \n",
    "* DEADLINE: JULY 14th++ \n",
    "\n",
    "#### Today\n",
    "\n",
    "* Paper Presentation \n",
    "\n",
    "* Marks\n",
    "\n",
    "* Alpha Go Story\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Paper Presentation: Florian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-level performance in first-person multiplayer games with population-based deep reinforcement learning\n",
    "\n",
    "Jaderberg, Czarnecki, Dunning et al. 2018 - https://arxiv.org/abs/1807.01281\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=dltN4MxV1RI\n",
    "\n",
    "* Can RL agents achieve human-like performance in complex real-time multiplayer games?\n",
    "* Specific task: Training agents to successfully play **Quake III**'s \"Capture the Flag\" mode in teams of two.\n",
    "\n",
    "***\n",
    "## What is Quake III?\n",
    "* First-person shooter; released in 1999 and extensively played in tournaments during the 2000s\n",
    "* Focus on quick movement and reflexes as well as game knowledge\n",
    "* **Capture the Flag**: Capturing the enemy team's flag from their base and carrying it to one's own base scores a point for the team\n",
    "\n",
    "### Modified version from DeepMind Lab\n",
    "* Randomly generated environments for each match\n",
    "* Heavily simplified user interface and graphics\n",
    "* \"Disc gadget\" is used to \"tag\" enemies and return them to their base after a delay\n",
    "\n",
    "***\n",
    "## Main problems\n",
    "* Large state space - four players, two flags, previously unknown environment\n",
    "* Large action space - free movement, tagging, flag interactions\n",
    "* Sparse rewards - scoring points, winning matches\n",
    "* Many possible strategies - offense, defense, base camping\n",
    "* Teamplay is necessary, but communication is difficult\n",
    "\n",
    "## General approach\n",
    "* Population of agents is trained by playing with and against each other\n",
    "* Maps are generated randomly for each match\n",
    "* State space is purely based on pixel input\n",
    "* Each agent is assigned an Elo rating based on its performance - agents are matched based on this rating\n",
    "* Weak agents are replaced with \"mutated\" versions of stronger agents\n",
    "\n",
    "![quake-pic1.png](https://d3i71xaburhd42.cloudfront.net/cd7447d87f939e2210e41e786908e12d89a8be21/3-Figure1-1.png)\n",
    "\n",
    "##  Learning methods\n",
    "* \"Multi-timescale recurrent neural network with external memory\"\n",
    "    * Fast RNN takes in an observation 15 times per second, selects an action\n",
    "        * Observations are purely based on the game's graphics and interface\n",
    "    * Slow RNN is activated less often, used for memory - informs action selection via \"latent stochastic variable\"\n",
    "* Two levels of optimisation:\n",
    "    * Outer optimisation uses population-based training\n",
    "        * tries to find out which actions lead to the \"meta-reward\" of winning matches\n",
    "        * sets internal rewards for different actions accordingly (\"FTW\")\n",
    "        * tweaks RL hyperparameters\n",
    "    * Inner optimisation uses RNN-based reinforcement learning\n",
    "        * focuses on maximising internal rewards\n",
    "* Performance of different agents is constantly compared\n",
    "    * Weak agents are replaced with \"mutated\" versions of stronger agents\n",
    "    * Which actions lead to winning matches?\n",
    "    \n",
    "![quake-pic2.png](https://d3i71xaburhd42.cloudfront.net/cd7447d87f939e2210e41e786908e12d89a8be21/6-Figure2-1.png)\n",
    "\n",
    "![quake-pic4.png](https://d3i71xaburhd42.cloudfront.net/cd7447d87f939e2210e41e786908e12d89a8be21/9-Figure4-1.png)\n",
    "\n",
    "## Results\n",
    "* The trained agents...\n",
    "    * consistently beat human players and achieve significantly higher Elo ratings\n",
    "    * remember the positions of flags, teammates and bases, even though they were not explicitly trained to\n",
    "    * show behavior and strategies commonly seen from human players - base camping, etc.\n",
    "    \n",
    "## Criticism\n",
    "* Heavily simplified version of the game\n",
    "    * simpler graphics - other players are much easier to see\n",
    "    * random maps - map knowledge does not play a role (unlike in the original game)\n",
    "    * smaller maps, less players, only one gun/gadget, no other items - state and action space would be even larger otherwise\n",
    "* Agents can react much faster and tag more accurately than humans\n",
    "* Skill level of human testers is unclear - were agents tested against professional players?\n",
    "    * \"average human\"\n",
    "    * \"strong human\"\n",
    "    * \"human participants with first-person video game experience\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) HOMEWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, the grading for the seminar is made up the following partial marks: \n",
    "a) 50% paper presentation + markdown handout\n",
    "b) 50% homework (distributed equally over 3 homeworks, means 16.67% per homework)\n",
    "\n",
    "Marks are sent to your private email adress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) QA, Discussion & Philosophy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Go\n",
    "\n",
    "The nature paper to the algorithm you can find here: https://www.nature.com/articles/nature16961\n",
    "\n",
    "The movie illustrating the story behind you can find here: https://www.youtube.com/watch?v=WXuK6gekU1Y\n",
    "\n",
    "I picked some scenes out of the movie which will help us understand the problem alpha go solves and how it does that as well as the novel questions that arise from these solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Game of Go\n",
    "\n",
    "How does go work? \n",
    "\n",
    "![alt_text](https://i.pinimg.com/originals/34/51/8d/34518d61d28210be1ac6f6116368c4a3.png \"\")\n",
    "\n",
    "https://www.youtube.com/watch?v=WXuK6gekU1Y&t=455s\n",
    "\n",
    "`From a philosophical standpoint, why are games so interesting to study?`\n",
    "\n",
    "`After Chess and DeepBlue, why is Go a good game?`\n",
    "\n",
    "`What would be an intersting game from an NLP perspective?`\n",
    "\n",
    "\n",
    "Scribbl\n",
    "\n",
    "Cards Against Humanity \n",
    "\n",
    "Codenames\n",
    "\n",
    "Taboo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Problem Description\n",
    "\n",
    "* Go has a full observable state space, that means it could be computationally solved (...)\n",
    "\n",
    "![alt_text](https://static.wixstatic.com/media/22ed14_2c663d2f34744841a938b0a3378f9e7e~mv2_d_4608_2592_s_4_2.jpg/v1/fill/w_1000,h_563,al_c,q_90,usm_0.66_1.00_0.01/22ed14_2c663d2f34744841a938b0a3378f9e7e~mv2_d_4608_2592_s_4_2.jpg \"\")\n",
    "\n",
    "**Why is go so hard?** \n",
    "\n",
    "Because the state space is incredibly vast and the branching factor after each move is immense (b~250, depth~150). The final number of states is approximately b^d.\n",
    "\n",
    "How can we transform the Go game into a reinforcement learning problem? \n",
    "\n",
    "STATE SPACE: all possible configurations of the 19*19 board game\n",
    "\n",
    "STATE SPACE: image of the 19*19 board game -> put into state representation matrix. \n",
    "\n",
    "ACTION SPACE: [x,y] coordinate on the board in the space of 19*19\n",
    "\n",
    "ACTION: [4,2]\n",
    "\n",
    "REWARD: [-1,1]\n",
    "\n",
    "What is the environment here? \n",
    "\n",
    "The environment is the ,,physics'' of the game, that means the RULES of the game of Go.  \n",
    "\n",
    "\n",
    "`Is it possible to put any problem into the reinforcement learning framework?`\n",
    "\n",
    "`Is it alwas useful? Why could it be dangerous?`\n",
    "\n",
    "Here are 2 controversary edge cases: \n",
    "\n",
    "Reward is Enough: https://www.semanticscholar.org/paper/Reward-is-enough-for-convex-MDPs-Zahavy-O'Donoghue/2654cf2ee6b4a34b04114a9f631ac375e15c2f8c\n",
    "\n",
    "The AI Economist: \n",
    "https://www.semanticscholar.org/paper/The-AI-Economist%3A-Improving-Equality-and-with-Tax-Zheng-Trott/25a1070c7d2cca21731d1427038c49be370525a2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Solution\n",
    "\n",
    "How does alpha go work internally? \n",
    "\n",
    "![alt_text](https://www.researchgate.net/profile/Pengju-Ren/publication/316618941/figure/fig9/AS:668465388068866@1536385976388/ntuitive-reasoning-of-AlphaGo-optimal-solution-so-that-the-computer-can-find-the.png \"\")\n",
    "\n",
    "https://www.youtube.com/watch?v=WXuK6gekU1Y&t=710s\n",
    "\n",
    "https://www.youtube.com/watch?v=WXuK6gekU1Y&t=2830s\n",
    "\n",
    "Taking a look into the paper: \n",
    "\n",
    "* 1. SL Policy Network: => Trained supervised based on human expert players (bootstrap). How is it trained? Input is the state representation -> output is an action -> backpropagation towards the human expert/gold standard output. \n",
    "\n",
    "\n",
    "* 2. RL Policy Network: => Take the pre-trained policy network and let it play against itself iteratively using POLIY GRADIENT METHODS. Yes, this is a sparse reward problem,.... but we have time and computational power, so we can do that. \n",
    "\n",
    "\n",
    "* 3. Value Network: => Take the RL improved policy network and bootstrap the VALUE NETWORK. The value network tells us how good our board state currently is and how the probability of winning is. The main idea to train the value network is, to take a self-play game sequence from the RL improved policy network and try to predict the value function for the state based and backpropagate based on the OUTCOME of the game (was the value evaluation correct or not). \n",
    "\n",
    "\n",
    "* 4. Use Both Networks to feed the MONTE CARLO TREE SEARCH, which allows us to PLAN the next moves of the game (=seeing into the future) based on the policy function, that can help us create sequences moves by sampling from it and the value function by using it for the evaluation of these board positions and then choosing the most promising path in that search tree. \n",
    "\n",
    "`Why is supervised learning useful to bootstrap alpha go in this setup?`\n",
    "\n",
    "`Looking into the future: In AlphaZero, Self-Play turns out to be efficient to train the agent completely WITHOUT human supervision. Do you think Self-Play has potential in an NLP context and how?`\n",
    "\n",
    "`What can we learn from the way alpha go works and how it behaves in competition with a human player considering the concept of intelligence in general?`\n",
    "\n",
    "The first stone (Computing the initial search tree):\n",
    "https://www.youtube.com/watch?v=WXuK6gekU1Y&t=1900s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creativity\n",
    "\n",
    "Within the game against lee sedol some interesting moves from both players happened. \n",
    "\n",
    "Alpha Go: Move 37\n",
    "\n",
    "https://www.youtube.com/watch?v=WXuK6gekU1Y&t=2950s\n",
    "\n",
    "Lee Sedol: Move 78\n",
    "\n",
    "https://www.youtube.com/watch?v=WXuK6gekU1Y&t=3932s\n",
    "\n",
    "![alt_text](https://cdn.redshift.autodesk.com/2017/05/what-is-machine-learning-header.jpg \"\")\n",
    "\n",
    "`In the game, AlphaGo came up with a novel move, never seen in Go before. Do you think this is creativity? Do you think human creativity differs from machine creativity?`\n",
    "\n",
    "`At the beginning of the seminar we spoke about the Turing Test as a kind of an intelligence test for a machine learning system. From an NLP perspective, what would be the AlphaGo moment to NLP?`\n",
    "\n",
    "`Lee Sedol quit the game of Go because the dominance of AI. What do you think about the dominance of AI in intelligence tasks? Is it dangerous? Is it an opportunity?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine Learning Seminar in WS 2021/2022\n",
    "\n",
    "Currently discussing the following:\n",
    "\n",
    "a) Machine Learning Visualization / Visual Machine Learning\n",
    "\n",
    "b) NLP in Visualization (Natural Language Interfaces, Machine Learning based Interaction Design)\n",
    "\n",
    "c) ...?\n",
    "\n",
    "I will send an email to all guys of the seminar if you are interested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for joining the seminar! \n",
    "\n",
    "# I wish you all the best for your upcoming exams!\n",
    "\n",
    "# Have a nice holiday!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
