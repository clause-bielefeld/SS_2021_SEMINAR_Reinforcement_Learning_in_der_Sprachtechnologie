{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34e8c20",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 09 Reinforcement Learning in der Sprachtechnologie\n",
    "## QA: Examples in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec7f02",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Announcements\n",
    "\n",
    "#### Papers\n",
    "\n",
    "\n",
    "#### Homework\n",
    "\n",
    "* Update: Only 3 Homeworks needed for this seminar\n",
    "        \n",
    "* NEW DEADLINE: JUNE 23th\n",
    "\n",
    "#### Today\n",
    "\n",
    "* taking a closer look at Q-Learning on two examples: 1. Discrete Environment (TAX), 2. Continous Environment (CartPole) and how to tackle those\n",
    "\n",
    "* QA\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59fe4b",
   "metadata": {},
   "source": [
    "### A) Paper Presentation I: Judith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de78bb8",
   "metadata": {},
   "source": [
    "***\n",
    "## Title: Ranking Sentences for Extractive Summarization with Reinforcement Learning \n",
    "(Shashi Narayan, Shay B. Cohen, Mirella Lapata, 2018)\n",
    "\n",
    "### Link: \n",
    "[Ranking Sentences for Extractive Summarization with Reinforcement Learning](https://www.semanticscholar.org/paper/Ranking-Sentences-for-Extractive-Summarization-with-Narayan-Cohen/59562be2cf8e01e8b7bb7560cef56158ea171227)\n",
    "\n",
    "\n",
    "### Summary:\n",
    "\n",
    "The authors conceptualize <ins>extractive summarization as a sentence ranking task and propose a new training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective.</ins> They use their algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms extractive and abstractive systems when evaluated automatically and by humans.\n",
    "\n",
    "\n",
    "***\n",
    "***\n",
    "### Problem/Task/Question:\n",
    "\n",
    "#### <font color='blue'>TASK</font>: Single document summarization\n",
    "+ =  producing a shorter version of a document while preserving its information content\n",
    "+  (abstractive summarization: involves various text rewriting operations (e.g., substitution, deletion, reordering))\n",
    "+ **extractive summarization**: identifying (and subsequently concatenating) the most important sentences in a document.\n",
    "\n",
    "\n",
    "\n",
    "#### <font color='blue'>MOTIVATION</font>: previous models for extractive summarization\n",
    "+ previous models (often) conceptualize <ins>extractive summarization as a sequence labeling task</ins> in which each label speciﬁes whether each document sentence should be included in the summary.\n",
    "+ These models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. \n",
    "+ They are <ins>typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels</ins> \n",
    "+ **the authors criticize that**: \n",
    " + these models do not learn to rank sentences based on their importance due to the absence of a ranking-based objective\n",
    " + there is a mismatch between the learning objective and the evaluation criterion, namely ROUGE, which takes the entire summary into account\n",
    " + models with cross-entropy training tend to generate wordy summaries with unnecessarily long sentences and redundant information\n",
    " \n",
    "***\n",
    "***\n",
    "\n",
    "### Solution/Idea/Model/Approach:\n",
    "\n",
    "#### <font color='blue'>SOLUTION</font>: Sentence Ranking with Reinforcement Learning\n",
    " + the authors adapt reinforcement learning to their formulation of extractive summarization to rank sentences for the summary generation: they <ins>globally optimize the ROUGE evaluation metric and learn to rank sentences for summary generation through a reinforcement learning objective</ins>\n",
    " + <ins> they train a model to predict the individual ROUGE score for each sentence in the document and then select the top m sentences with highest scores.</ins> <font color='grey'>(instead of maximizing the likelihood of the ground-truth labels with cross-entropy loss (like the previous models did))</font>\n",
    " +  they claim that this framework would make extractive models better at discriminating among sentences for the summarization task \n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "#### <font color='blue'>MODEL</font>: REFRESH  (REinFoRcement Learning-based Extractive Summarization)\n",
    "\n",
    "\n",
    "#### -- Summarization as sentence ranking: \n",
    "+ <ins> a sentence is ranked high for selection if it often occurs in high scoring summaries</ins>\n",
    "+ In detail: Given a document $D$ consisting of a sequence of sentences $(s_1,s_2 ,...,s_n )$, an extractive summarizer aims to produce a summary $S$ by selecting $m$ sentences from $D$ (where $m < n$).\n",
    "+ For each sentence $s_i ∈ D$:\n",
    "  + a  label $y_i  ∈ \\{0,1\\}$ is predicted (where 1 means that $s_i$ should be included in the summary) \n",
    "  + and a score $p(y_i|s_i,D , θ)$ is assigned, quantifying $s_i$’s relevance to the summary <font color='grey'>(Model parameters are denoted by $θ$)</font>\n",
    "+ They estimate $p(y_i|s_i,D , θ)$ using a neural network model and assemble a summary $S$ by selecting $m$ sentences with top $p(1|s_i,D , θ)$ scores.\n",
    " \n",
    " \n",
    "#### -- Components of the model:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/figure_1.png \"\")\n",
    "\n",
    "+ **sentence encoder** (the core component): a convolutional neural network (CNN) to encode sentences into continuous representations --> returns sentence embeddings \n",
    " +  *<font color='grey'>they use temporal narrow convolution by applying a kernel ﬁlter $K$ of width $h$ to a window of $h$ words in sentence $s$ to produce a new feature. This ﬁlter is applied to each possible window of words in $s$ to produce a feature map $f ∈ R^{k−h+1}$ where $k$ is the sentence length. They then apply max-pooling over time over the feature map $f$ and take the maximum value as the feature corresponding to this particular ﬁlter $K$. They  use multiple kernels of various sizes and each kernel multiple times to construct the representation of a sentence.* \n",
    " + *<font color='grey'>In the figure, kernels of size 2 (red) and 4 (blue) are applied three times each. Max-pooling over time yields two feature lists $f^{K_2}$ and $f^{K_4}∈ R^3$ . The ﬁnal sentence embeddings have six dimensions.*\n",
    "\n",
    "\n",
    "+ **document encoder**: a recurrent neural network (RNN) to compose a sequence of sentences to obtain a document representation\n",
    " + *<font color='grey'>they feed sentences in reverse order to make sure that the network also considers the top sentences of the document which are particularly important for summarization*\n",
    " + *<font color='grey'> is implemented with LSTM (Long Short-Term Memory) to avoid the vanishing gradient problem when training long sequences*\n",
    " \n",
    "\n",
    "+ **sentence extractor**: another RNN to sequentially label each sentence in a document with 1 (relevant for the summary) or 0 (otherwise) \n",
    " + *<font color='grey'>It is implemented with LSTM cells and a softmax layer*  \n",
    " + *<font color='grey'>reads the sentence and makes a binary prediction, conditioned on the document representation (obtained from the document encoder) and the previously labeled sentences. This way, the sentence extractor is able to identify locally and globally important sentences within the document.*\n",
    " + *<font color='grey'>they rank the sentences in a document $D$ by $p(y_i = 1 | s_i ,D,θ)$, which is the conﬁdence scores assigned by the softmax layer of the sentence extractor.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "#### -- they cast this summarization model in the <font color='blue'>REINFORCEMENT LEARNING PARADIGM</font>: \n",
    " + the **model** can be viewed as the <font color='green'>**agent**</font> which interacts with an <font color='green'>**environment**</font> consisting of **documents** \n",
    " + the <font color='green'>**actions**</font> are **relevance scores** which lead to sentence ranking\n",
    " \n",
    " \n",
    " + the training algorithm performs sentence ranking using **ROUGE** as the <font color='green'>**reward function**</font>.\n",
    "   + As the reward function they use mean $F_1$ of ROUGE-1, ROUGE-2, and ROUGE-L.\n",
    "   + *<font color='grey'> ROUGE-1 and ROUGE-2 measure unigram and bigram overlap between model output and reference (gold standard summary) and are meant to assess informativeness*\n",
    "   + *<font color='grey'> ROUGE-L measures the longest common subsequence between model output and referenceis and is meant to assess ﬂuency*\n",
    " + the algorithm explores the space of candidate summaries while learning to optimize the reward function\n",
    " + During training, the model combines the maximum-likelihood cross-entropy loss with rewards from policy gradient reinforcement learning to directly optimize the evaluation metric relevant for the summarization task\n",
    "  \n",
    "\n",
    "\n",
    " + **<font color='green'>Policy Learning**</font>:\n",
    " + **<font color='green'>1.</font> At ﬁrst, the agent is initialized randomly, it reads document $D$ and predicts a relevance score for each sentence $s_i ∈ D$ using policy $p(y_i | s_i, D, θ)$**\n",
    " + **<font color='green'>2.</font>  Once the agent is done reading the document, a summary with labels $\\hat{y}$ is sampled out of the ranked sentences.**\n",
    " + **<font color='green'>3.</font>  The agent is then given a reward $r$ according to how well the extract resembles the gold-standard summary (with the reward function).**\n",
    " \n",
    " \n",
    " + **They update the agent using the [REINFORCE algorithm (Williams, 1992)](https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)** which aims to minimize the negative expected reward: $L(θ) = –E_{\\hat{y}∼p_θ}[r(\\hat{y})]$ <font color='grey'>($p_θ$ stands for $p(y |D, θ)$)</font>\n",
    "   + <font color='grey'>REINFORCE is based on the observation that the expected gradient of a non-differentiable reward function (ROUGE, in this case) can be computed as follows: $∇L(θ) = –E_{\\hat{y}∼p_θ}[r(\\hat{y})∇log p(\\hat{y}|D,θ]$\n",
    "   + The authors approximate the expected gradient using a single sample $\\hat{y}$ from $p_θ$ for each training example in a batch:\n",
    "   + $∇L(θ) ≈ r(\\hat{y})∇log p(\\hat{y}|D,θ) \n",
    "   ≈ r(\\hat{y}) \\sum_{i=1}^{n}∇log p(\\hat{y_i}|s_i,D,θ)$*\n",
    "   + they limit the search space of $\\hat{y}$ in this Equation to the set of largest probability samples $\\hat{Y}$: They approximate $\\hat{Y}$ by the k extracts which receive highest ROUGE scores</font>\n",
    " + the objective is to learn to discriminate among sentences with respect to how often they occur in high scoring summaries\n",
    " \n",
    " \n",
    " + **In other words**: \n",
    " + they assemble candidate summaries by ﬁrst selecting $p$ sentences from the document which on their own have high ROUGE scores. They then generate all possible combinations of $p$ sentences subject to maximum length $m$ and evaluate them against the gold summary.\n",
    "    + Summaries are ranked according to $F_1$ by taking the mean of ROUGE-1, ROUGE-2, and ROUGE-L. \n",
    "    + $\\hat{Y}$ contains these top $k$ candidate summaries. During training, they sample $\\hat{y}$  from $\\hat{Y}$ instead of $p(\\hat{y}|D, θ)$\n",
    "\n",
    "     \n",
    "   \n",
    "+ As a result, **reinforcement learning helps extractive summarization in two ways**: \n",
    "  + **(a)** it directly optimizes the evaluation metric instead of maximizing the likelihood of the ground-truth label\n",
    "  + **(b)** it makes their model better at discriminating among sentences: a sentence is ranked high for selection if it often occurs in high scoring summaries\n",
    "    \n",
    "\n",
    "\n",
    "***\n",
    "#### <font color='blue'>EXPERIMENTAL SETUP</font>: \n",
    "\n",
    "#### -- Datasets:\n",
    "+ evaluated their model on the <ins> CNN and DailyMail news highlights datasets</ins> (these have been used as testbeds for the evaluation of neural summarization systems) and did not anonymize entities or lower case tokens\n",
    "+ assumed that the <ins>“story highlights” associated with each article are gold-standard abstractive summaries </ins>\n",
    "  + during training they are used to generate high scoring extracts and to estimate rewards for them\n",
    "  + during testing, they are used as reference summaries to evaluate the model\n",
    "\n",
    "\n",
    "#### <font color='grey'>-- Some Implementation Details:\n",
    "+ <font color='grey'> generated extracts by selecting three sentences (m = 3) for CNN articles and four sentences (m = 4) for DailyMail articles (because gold highlights in the CNN/DailyMail validation sets are 2.6/4.2 sentences long)\n",
    "+ For both datasets, they estimated high-scoring extracts using 10 document sentences (p = 10) with highest ROUGE scores\n",
    "+ They used the One Billion Word Benchmark corpus with the skip-gram model to train word embeddings.\n",
    "+ For the sentence encoder, they used a list of kernels of widths 1 to 7, each with output channel size of 50. The sentence embedding size in their model was 350.\n",
    "+ For the recurrent neural network component in the document encoder and sentence extractor, they used a single-layered LSTM network with size 600. \n",
    "+ they performed minibatch cross-entropy training with a batch size of 20 documents for 20 training epochs. After each epoch, they evaluated their model on the validation set and chose the best performing model for the test set. \n",
    "+ Their system is implemented in TensorFlow*\n",
    "\n",
    "***\n",
    "***\n",
    "### Main Results/Findings: \n",
    "\n",
    "#### <font color='blue'>AUTOMATIC EVALUATION</font>: \n",
    "\n",
    "+ report F1 ROUGE to evaluate summarization quality \n",
    "+ report ROUGE-1 and ROUGE-2 (unigram and bigram overlap) to assess informativeness \n",
    "+ report ROUGE-L (longest common subsequence) to assess ﬂuency\n",
    "\n",
    "\n",
    "+ compare their model REFRESH against:\n",
    "  + a baseline which simply selects the ﬁrst leading sentences from each document (LEAD)\n",
    "  + two extractive neural models which are both trained with cross-entropy loss:\n",
    "    + Cheng and Lapata (2016) (who train on individual labels; i.e. one sentence, one label)\n",
    "    + Nallapati et al. (2017) (who use collective labels; i.e. one label for a few sentences)\n",
    "  + the abstractive systems of Chen et al. (2016), Nallapati et al. (2016), See et al. (2017), and Tan and Wan (2017).\n",
    "\n",
    "\n",
    "#### -- Results:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/table_2.png \"\")\n",
    "\n",
    "\n",
    "+ The results in this Table2 show that REFRESH is superior to the LEAD baseline and extractive systems across datasets and metrics. \n",
    "+ their model „outperforms“ both extractive and abstractive systems\n",
    "+ results show that cross-entropy training is not well-suited to the summarization task\n",
    "+ state-of-the-art abstractive systems lag behind extractive ones when the latter are globally trained\n",
    "\n",
    "\n",
    "\n",
    "#### <font color='blue'>TWO HUMAN EVALUATIONS</font>: \n",
    "\n",
    "\n",
    "\n",
    "#### -- Experiment 1: to assess which type of summary participants prefer\n",
    "\n",
    "+ they <ins>compared extractive and abstractive systems: Participants read the articles and were asked to rank the summaries from best (1) to worst (4) in order of informativeness and ﬂuency.</ins>\n",
    "+ participants were presented with a news article and summaries generated by these systems: the LEAD baseline, abstracts from See et al. (2017) (abstractive) and extracts from REFRESH and also. The authors also included the human-authored highlights (GOLD). \n",
    "+ The authors randomly selected 10 articles from the CNN test set and 10 from the DailyMail test set. The study was completed by ﬁve participants, all native or proﬁcient English speakers. Each participant was presented with the 20 articles. The order of summaries to rank was randomized per article and the order of articles per participant.\n",
    "\n",
    "#### -- Experiment 2: to assess how much key information from the document is preserved in the summary\n",
    "\n",
    "+ <ins> followed a question-answering (QA) paradigm </ins> and created a set of questions based on the gold summary\n",
    "+ They <ins> examined whether participants were able to answer these questions by reading system summaries alone</ins> (the more questions a system can answer, the better it is at summarizing the document as a whole\n",
    "+ They used the same 20 documents  as in the ﬁrst elicitation study. The authors wrote multiple factbased question-answer pairs for each gold summary without looking at the document.\n",
    "+ Five participants were shown summaries from three systems: the LEAD baseline, the abstractive system of See et al. (2017), and REFRESH. \n",
    "+ scoring mechanism: a correct answer was marked with a score of one, partially correct answers with a score of 0.5, and zero otherwise. The ﬁnal score for a system is the average of all its question scores. \n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/figure_2.png \"\")\n",
    "\n",
    "\n",
    "\n",
    "#### -- Results:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/table_3.png \"\")\n",
    "\n",
    "+ **experiment 1**: REFRESH is ranked second best (after GOLD)\n",
    "+ **experiment 2**: REFRESH is ranked best (based on summaries generated by REFRESH, participants can answer 66.34% of questions correctly).\n",
    "+ this „overwhelmingly“ shows that human subjects ﬁnd the summaries produced bei REFRESH more informative and complete.\n",
    "\n",
    "\n",
    "***\n",
    "***\n",
    "### Critical Discussion: \n",
    "\n",
    "+ <font color='green'>**+**</font> the article is written understandable and with a clear structure\n",
    "+ <font color='green'>**+**</font> In my opinion the authors have explained in an comprehensible way why they handle extractive summarization differently from previous models \n",
    "+ <font color='green'>**+**</font> I liked that they examined both automatic and human evaluation\n",
    "+ <font color='green'>**+**</font>  ... \n",
    "\n",
    "\n",
    "+ <font color='red'>**-**</font> only one small example for a summary produced by REFRESH was given \n",
    "+ <font color='red'>**-**</font> only five participants were chosen for evaluation and we don’t get a lot of information about the participants (such as age, gender, relationship to the authors etc.)\n",
    "+ <font color='red'>**-**</font> the authors authors wrote the questions for experiment 2 themselves (even if they did it \"without looking at the document\", I still find this questionable)\n",
    "+ <font color='red'>**-**</font>  ... \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75dde4",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547954f",
   "metadata": {},
   "source": [
    "## C) Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce9daa",
   "metadata": {},
   "source": [
    " **RL in Python**\n",
    "\n",
    "In this seminar, we will take a closer look at two examples. \n",
    "\n",
    "The first example deals with the TAXI environment, which is an environment with a **discrete state space**.\n",
    "\n",
    "The second example deals with the CARTPOLE environment, which is an environment with a **continous state space**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e54aa4",
   "metadata": {},
   "source": [
    "### Taxi-V3 Environment\n",
    "\n",
    "Find the environment here: https://gym.openai.com/envs/Taxi-v3/\n",
    "\n",
    "For solving the Taxi environment, Q-Learning with a discrete Q-Table can be applied. \n",
    "\n",
    "In the following there are 2 examples shown. \n",
    "\n",
    "Example 1 is a simple, straight forward implementation of the learning algorithm NOT FOLLOWING the openai gym interfaces. \n",
    "\n",
    "Example 2 is an implementation following the interfaces given by openai gym, which are widely accepted within the commumity. The advantage of following a programming style due to those interfaces is, that you can test and run your agents on different environments literally in a plug and play mode. \n",
    "\n",
    "**MY TIPP FOR YOU:** If you are new to reinforcement learning and object oriented programming at all, try to implement the simpler version first and then try to understand the more abstract version. In you homework it is absolutely okay if you solve an environment with a simple, straight forward implementation of the Q-Learning Algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7628ae",
   "metadata": {},
   "source": [
    "### Straight Forward Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58dc5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "import random \n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d651cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an instance of the frozen lake gym environment\n",
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4dd5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "6\n",
      "Discrete(500)\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "print(action_space_size)\n",
    "state_space = env.observation_space \n",
    "print(state_space)\n",
    "state_space_size = env.observation_space.n\n",
    "print(state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad34166c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Q-TABLE\n",
    "# build our action-value table | Q-TABLE\n",
    "# as you already know, the q-table looks like this\n",
    "# state | action_space\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "#print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d29ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "num_test_episodes = 3     \n",
    "\n",
    "# q-learning | update parameters\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# exploration-exploitation trade off\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b051e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-LEARNING\n",
    "\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = env.reset()\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > exploration_rate):\n",
    "            action = np.argmax(q_table[state, : ])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-Table Q(s,a) using the bellman update  \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, : ]))\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aabc6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****INFO: average reward per thousand episodes: ***** \n",
      "\n",
      "1000 :  -251.62699999999995\n",
      "2000 :  -40.56400000000015\n",
      "3000 :  2.023999999999995\n",
      "4000 :  5.467999999999978\n",
      "5000 :  6.982999999999963\n",
      "6000 :  7.306999999999973\n",
      "7000 :  7.374999999999958\n",
      "8000 :  7.208999999999966\n",
      "9000 :  7.34199999999997\n",
      "10000 :  7.370999999999965\n",
      "\n",
      "\n",
      " ***** Q-TABLE ***** \n",
      "\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-3.58354789  1.50738533 -2.67946757 -0.72508374  9.6220697  -7.85762471]\n",
      " [ 3.24488086  8.5430262   0.90119722  6.05827363 14.11880599 -3.88260399]\n",
      " ...\n",
      " [-1.09370698  4.21394742 -1.13587692 -1.42989285 -8.20037517 -7.85013748]\n",
      " [-3.08914083 -3.04936186 -3.26431454  5.38236448 -9.84217015 -9.37361561]\n",
      " [-0.23842085 -0.31662019  0.12178955 16.27783037 -2.68370758 -1.24277331]]\n"
     ]
    }
   ],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print our learned q-table\n",
    "print(\"\\n\\n ***** Q-TABLE ***** \\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66eb441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "INFO: ***** agent did not reach the goal.\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1: # check reward from environment for correct display\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent did not reach the goal.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911765af",
   "metadata": {},
   "source": [
    "### OpenAI Interface Conform Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e01e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN = the data_object (JSON SERIALIZABLE)\n",
    "class Environment():\n",
    "    \"\"\"The main Environment class. It encapsulates an environment with\n",
    "    arbitrary behind-the-scenes dynamics. An environment can be\n",
    "    partially or fully observed.\n",
    "    The main API methods that users of this class need to know are:\n",
    "        step\n",
    "        reset\n",
    "        render\n",
    "        close\n",
    "        seed\n",
    "    And set the following attributes:\n",
    "        action_space: The Space object corresponding to valid actions\n",
    "        observation_space: The Space object corresponding to valid observations\n",
    "        reward_range: A tuple corresponding to the min and max possible rewards\n",
    "    Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n",
    "    The methods are accessed publicly as \"step\", \"reset\", etc...\n",
    "    \"\"\"\n",
    "    # Set this in SOME subclasses\n",
    "    metadata = {'render.modes': []}\n",
    "    reward_range = (-float('inf'), float('inf'))\n",
    "    spec = None\n",
    "\n",
    "    def __init__(self, action_space=None, observation_space=None):\n",
    "        # Set variables\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "    \n",
    "# REPOSITORY = the functionality interface\n",
    "class EnvironmentRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        Args:\n",
    "            action (object): an action provided by the agent\n",
    "        Returns:\n",
    "            observation (object): agent's observation of the current environment\n",
    "            reward (float) : amount of reward returned after previous action\n",
    "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to an initial state and returns an initial\n",
    "        observation.\n",
    "        Note that this function should not reset the environment's random\n",
    "        number generator(s); random variables in the environment's state should\n",
    "        be sampled independently between multiple calls to `reset()`. In other\n",
    "        words, each call of `reset()` should yield an environment suitable for\n",
    "        a new episode, independent of previous episodes.\n",
    "        Returns:\n",
    "            observation (object): the initial observation.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.) By convention,\n",
    "        if mode is:\n",
    "        - human: render to the current display or terminal and\n",
    "          return nothing. Usually for human consumption.\n",
    "        - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
    "          representing RGB values for an x-by-y pixel image, suitable\n",
    "          for turning into a video.\n",
    "        - ansi: Return a string (str) or StringIO.StringIO containing a\n",
    "          terminal-style text representation. The text can include newlines\n",
    "          and ANSI escape sequences (e.g. for colors).\n",
    "        Note:\n",
    "            Make sure that your class's metadata 'render.modes' key includes\n",
    "              the list of supported modes. It's recommended to call super()\n",
    "              in implementations to use the functionality of this method.\n",
    "        Args:\n",
    "            mode (str): the mode to render with\n",
    "        Example:\n",
    "        class MyEnv(Env):\n",
    "            metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "            def render(self, mode='human'):\n",
    "                if mode == 'rgb_array':\n",
    "                    return np.array(...) # return RGB frame suitable for video\n",
    "                elif mode == 'human':\n",
    "                    ... # pop up a window and render\n",
    "                else:\n",
    "                    # just raise an exception\n",
    "                    super(MyEnv, self).render(mode=mode)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        \"\"\"Override close in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4970a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPOSITORY IMPLEMENTATION = the way how you would like to implement it\n",
    "class EnvironmentRepositoryImpl(EnvironmentRepository):\n",
    "    # Initialize / Instance Attributes\n",
    "    def __init__(self, environment):\n",
    "        # Set variables\n",
    "        self.data_object = environment\n",
    "        print('Environment initialized')\n",
    "\n",
    "    def step(self, action):\n",
    "        state = self.data_object.step(action)\n",
    "        return state\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.data_object.reset()\n",
    "        return state\n",
    "\n",
    "    def render(self):\n",
    "        self.data_object.render()\n",
    "\n",
    "    def close(self):\n",
    "        state = self.data_object.close()\n",
    "\n",
    "    def get_action_space(self):\n",
    "        # get action space from api of the playground or via js in browser using selenium\n",
    "        action_space = self.data_object.action_space\n",
    "        return action_space\n",
    "\n",
    "    def get_observation_space(self):\n",
    "        # get observation space of the playground from api or via js in browser using selenium\n",
    "        observation_space = self.data_object.observation_space\n",
    "        return observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d386fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN\n",
    "class Agent():\n",
    "    # class variables\n",
    "    agent_variable = \"\"\n",
    "    # class methods\n",
    "    def __init__(self, agent_variable=\"\"):\n",
    "        self.agent_variable = agent_variable\n",
    "        \n",
    "# REPOSITORY\n",
    "class AgentRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def get_action(self, state): # This is the agent's POLICY, if you will\n",
    "        \"\"\" Agent gets a state as input and returns an action \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ad9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPOSITORY IMPLEMENTATION\n",
    "class AgentRepositoryImpl(AgentRepository):\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, environment, agent):\n",
    "        # Set variables\n",
    "        self.data_object = agent\n",
    "        self.environment = environment\n",
    "        self.action_space = self.environment.action_space\n",
    "        self.observation_space = self.environment.observation_space\n",
    "        self.action_space_size = self.environment.action_space.n\n",
    "        self.observation_space_size = self.environment.observation_space.n\n",
    "        # INIT Q-TABLE\n",
    "        self.q_table = np.zeros((self.observation_space_size, self.action_space_size))\n",
    "        # INIT AGENT PARAMETERS\n",
    "        self.learning_rate = 0.7           # Learning rate\n",
    "        self.discount_rate = 0.618         # Discounting rate\n",
    "        self.exploration_rate = 1.0        # Exploration rate\n",
    "        self.max_exploration_rate = 1.0    # Exploration probability at start\n",
    "        self.min_exploration_rate = 0.01   # Minimum exploration probability \n",
    "        self.exploration_decay_rate = 0.01 # Exponential decay rate for exploration probability\n",
    "        print('Agent initialized.')\n",
    "\n",
    "    def get_action(self, state):\n",
    "        #EXPLORATION-EXPLOITATION TRADE OFF\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > self.exploration_rate):\n",
    "            # get action from q table\n",
    "            action = np.argmax(self.q_table[state, : ])\n",
    "        else:\n",
    "            # get random action\n",
    "            action = self.get_random_action()\n",
    "        return action\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        #action_set = random.sample(self.action_space, 1)\n",
    "        #action = action_set[0]\n",
    "        action = self.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, new_state):\n",
    "        self.q_table[state, action] = self.q_table[state, action] * (1 - self.learning_rate) + self.learning_rate * (reward + self.discount_rate * np.max(self.q_table[new_state, : ]))\n",
    "        \n",
    "    def update_exploration_rate(self, episode_num):\n",
    "        self.exploration_rate = self.min_exploration_rate + (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate*episode_num)\n",
    "    \n",
    "    def get_exploit_action(self, state):\n",
    "        action = np.argmax(self.q_table[state, : ])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f493b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 50000        # Total episodes\n",
    "max_steps_per_episode = 100 # Max steps per episode\n",
    "num_test_episodes = 5     # Total test episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e74c5ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "6\n",
      "Discrete(500)\n",
      "500\n",
      "(-inf, inf)\n",
      "Environment initialized\n",
      "Agent initialized.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the Environment\n",
    "\n",
    "# get the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "print(action_space_size)\n",
    "observation_space = env.observation_space \n",
    "print(observation_space)\n",
    "observation_space_size = env.observation_space.n\n",
    "print(observation_space_size)\n",
    "reward_range = env.reward_range\n",
    "print(reward_range)\n",
    "\n",
    "environment_data_object = env #Environment(action_space, observation_space)\n",
    "environment = EnvironmentRepositoryImpl(environment_data_object)\n",
    "\n",
    "# Setting up the Agent\n",
    "agent_data_object = Agent()\n",
    "agent = AgentRepositoryImpl(environment_data_object, agent_data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eb7459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = environment.reset()\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        action = agent.get_action(state)\n",
    "            \n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "        \n",
    "        # Update Q-Table Q(s,a) using the bellman update  \n",
    "        agent.update_q_table(state, action, reward, new_state)\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    agent.update_exploration_rate(episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78277361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****INFO: average reward per thousand episodes: ***** \n",
      "\n",
      "1000 :  -45.841999999999764\n",
      "2000 :  7.287999999999954\n",
      "3000 :  7.259999999999967\n",
      "4000 :  7.023999999999967\n",
      "5000 :  7.145999999999961\n",
      "6000 :  7.245999999999962\n",
      "7000 :  7.203999999999955\n",
      "8000 :  7.099999999999966\n",
      "9000 :  7.40999999999997\n",
      "10000 :  7.340999999999965\n",
      "11000 :  7.515999999999975\n",
      "12000 :  7.4629999999999646\n",
      "13000 :  7.3719999999999715\n",
      "14000 :  7.333999999999969\n",
      "15000 :  7.510999999999968\n",
      "16000 :  7.267999999999974\n",
      "17000 :  7.360999999999959\n",
      "18000 :  7.371999999999952\n",
      "19000 :  7.421999999999969\n",
      "20000 :  7.513999999999961\n",
      "21000 :  7.554999999999962\n",
      "22000 :  7.2889999999999615\n",
      "23000 :  7.44399999999997\n",
      "24000 :  7.575999999999966\n",
      "25000 :  7.469999999999965\n",
      "26000 :  7.227999999999968\n",
      "27000 :  7.430999999999969\n",
      "28000 :  7.321999999999962\n",
      "29000 :  7.374999999999972\n",
      "30000 :  7.409999999999963\n",
      "31000 :  7.292999999999961\n",
      "32000 :  7.396999999999964\n",
      "33000 :  7.383999999999964\n",
      "34000 :  7.428999999999962\n",
      "35000 :  7.465999999999966\n",
      "36000 :  7.630999999999972\n",
      "37000 :  7.214999999999956\n",
      "38000 :  7.302999999999965\n",
      "39000 :  7.465999999999969\n",
      "40000 :  7.49299999999997\n",
      "41000 :  7.343999999999957\n",
      "42000 :  7.376999999999977\n",
      "43000 :  7.35599999999996\n",
      "44000 :  7.327999999999954\n",
      "45000 :  7.5779999999999585\n",
      "46000 :  7.48499999999996\n",
      "47000 :  7.508999999999955\n",
      "48000 :  7.434999999999969\n",
      "49000 :  7.430999999999963\n",
      "50000 :  7.4589999999999606\n",
      "\n",
      "\n",
      " ***** Q-TABLE ***** \n",
      "\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -2.50376382  -2.43400548  -2.50421462  -2.43401698  -2.32039715\n",
      "  -11.43347974]\n",
      " [ -1.83913404  -1.35982324  -1.83910212  -1.35782468  -0.57891593\n",
      "  -10.36326322]\n",
      " ...\n",
      " [ -2.0811255   -1.96039066  -2.13516767  -1.35790145 -10.74418226\n",
      "  -10.67182998]\n",
      " [ -2.32311559  -2.36659774  -2.37434138  -2.13699863 -10.54004306\n",
      "   -9.89878125]\n",
      " [ -1.366666    -0.91        -1.21282     11.36        -9.73\n",
      "   -7.        ]]\n"
     ]
    }
   ],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print our learned q-table\n",
    "print(\"\\n\\n ***** Q-TABLE ***** \\n\")\n",
    "print(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33028e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "INFO: ***** agent reached the goal. *****\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        environment.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = agent.get_exploit_action(state)\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            environment.render()\n",
    "            if reward == 20:\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent missed the goal.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16959d8",
   "metadata": {},
   "source": [
    "### CartPole Environment\n",
    "\n",
    "Find the environment here: https://gym.openai.com/envs/CartPole-v0/\n",
    "\n",
    "For solving the CartPole environment, Q-Learning with two different approaches can be applied. \n",
    "\n",
    "The first approach would be to **discretize** the state space of the environment and based on this build a Q-Table following the Q-Learning algorithm. \n",
    "\n",
    "The second approach would be to **approximate** the Q-table with a neural network, due to the fact that a continous state space would lead to an infinite q-table. \n",
    "\n",
    "In the following there are 2 examples shown. \n",
    "\n",
    "Example 1 is an implementation of the Q-Learning algorithm using a discretization of the state space of the cartpole environment. \n",
    "\n",
    "Example 2 is an implementation of the Q-Learning algorithm that approximates the (infinite) q-table with a neural network. \n",
    "\n",
    "**MY TIPP FOR YOU:** If you are new to reinforcement learning and object oriented programming try to discretize your environments and use a simple, straigh forward implementation of the Q-Learning environment. If you are an advanced programmer and familiar to concepts of RL and ML when dealing with a continous state space of your environment use a neural network for approximating the q-table and build it into an agent which fits the openai interfaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcaebef",
   "metadata": {},
   "source": [
    "#### Discretization Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3d4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "import random \n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68dcb47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an instance of the frozen lake gym environment\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778f8446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "2\n",
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-676f4b2ed3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstate_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstate_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "print(action_space_size)\n",
    "state_space = env.observation_space \n",
    "print(state_space)\n",
    "state_space_size = env.observation_space.n\n",
    "print(state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f773156",
   "metadata": {},
   "outputs": [],
   "source": [
    "?env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f514316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-3.42857143, -2.05714286, -0.68571429,  0.68571429,  2.05714286,\n",
      "        3.42857143]), array([-2.14285714, -1.28571429, -0.42857143,  0.42857143,  1.28571429,\n",
      "        2.14285714]), array([-0.35714286, -0.21428571, -0.07142857,  0.07142857,  0.21428571,\n",
      "        0.35714286]), array([-1.42857143, -0.85714286, -0.28571429,  0.28571429,  0.85714286,\n",
      "        1.42857143])]\n",
      "6\n",
      "2401\n",
      "[ 0.04595299  0.03132225 -0.03783019  0.04184446]\n",
      "0.045952988930203595\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "# State Discretizer\n",
    "def discretize_range(lower_bound, upper_bound, num_bins):\n",
    "    return np.linspace(lower_bound, upper_bound, num_bins + 1)[1:-1]\n",
    "\n",
    "# Discretize the continuous state space for each of the 4 features.\n",
    "num_discretization_bins = 7\n",
    "state_bins = [\n",
    "    # Cart position.\n",
    "    discretize_range(-4.8, 4.8, num_discretization_bins),\n",
    "    # Cart velocity.\n",
    "    discretize_range(-3.0, 3.0, num_discretization_bins),\n",
    "    # Pole angle.\n",
    "    discretize_range(-0.5, 0.5, num_discretization_bins),\n",
    "    # Tip velocity.\n",
    "    discretize_range(-2.0, 2.0, num_discretization_bins)\n",
    "]\n",
    "print(state_bins)\n",
    "max_bins = max(len(bin) for bin in state_bins)\n",
    "print(max_bins)\n",
    "num_states = (max_bins + 1) ** len(state_bins)\n",
    "print(num_states)\n",
    "# our state space therefore has a size of 7x7x7x7 ~2500\n",
    "state_space_size = num_states\n",
    "\n",
    "# lets see how that looks like in practice\n",
    "state = env.reset()\n",
    "print(state)\n",
    "print(state[0])\n",
    "\n",
    "# discretize a given state into our state model, found here: https://www.statology.org/numpy-digitize/\n",
    "def discretize_value(value, bins):\n",
    "    return np.digitize(x=value, bins=bins)\n",
    "\n",
    "def discretize_state(observation):\n",
    "        # Discretize the observation features and reduce them to a single integer\n",
    "        #for i, feature in enumerate(observation):\n",
    "            #getting the bins\n",
    "            #print(state_bins[i]) \n",
    "            # extending to the max number of bins\n",
    "            #print(state_bins[i] * ((max_bins + 1)))\n",
    "            # putting it into the power of the decimal place \n",
    "            #print(state_bins[i] * ((max_bins + 1)) ** i)\n",
    "            # this encoding enables us to reach that: first states encodes one potency, second state encodes ten potency, third state encodes hundred potency, ...\n",
    "            #print(discretize_value(feature, state_bins[i]) * ((max_bins + 1) ** i))\n",
    "            \n",
    "        state = sum(\n",
    "            discretize_value(feature, state_bins[i]) * ((max_bins + 1) ** i)\n",
    "            for i, feature in enumerate(observation)\n",
    "        )\n",
    "        return state\n",
    "    \n",
    "test = discretize_state(state)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ec903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-TABLE\n",
    "# build our action-value table | Q-TABLE\n",
    "# as you already know, the q-table looks like this\n",
    "# state | action_space\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "#print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "num_test_episodes = 1     \n",
    "\n",
    "# q-learning | update parameters\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# exploration-exploitation trade off\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-LEARNING\n",
    "\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = env.reset()\n",
    "    # discretize state\n",
    "    state = discretize_state(state)\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > exploration_rate):\n",
    "            action = np.argmax(q_table[state, : ])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # discretize the observed state\n",
    "        new_state = discretize_state(new_state)\n",
    "        \n",
    "        # Update Q-Table Q(s,a) using the bellman update  \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, : ]))\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print our learned q-table\n",
    "print(\"\\n\\n ***** Q-TABLE ***** \\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    # discretize state\n",
    "    state = discretize_state(state)\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # discretize new state\n",
    "        new_state = discretize_state(new_state)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1: # check reward from environment for correct display\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent did not reach the goal.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbadbee",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bf86e",
   "metadata": {},
   "source": [
    "#### Deep Q Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd81b9",
   "metadata": {},
   "source": [
    "![alt_text](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1643ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an instance of the frozen lake gym environment\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4049c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "print(action_space_size)\n",
    "state_space = env.observation_space \n",
    "print(state_space)\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "print(state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce786da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "num_test_episodes = 1     \n",
    "\n",
    "# q-learning | update parameters\n",
    "discount_rate = 0.99\n",
    "\n",
    "# exploration-exploitation trade off\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "# q network parameters\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c272c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork():\n",
    "    ''' Deep Q Neural Network Class. '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):\n",
    "            self.criterion = torch.nn.MSELoss()\n",
    "            self.model = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(state_dim, hidden_dim),\n",
    "                            torch.nn.LeakyReLU(),\n",
    "                            torch.nn.Linear(hidden_dim, hidden_dim*2),\n",
    "                            torch.nn.LeakyReLU(),\n",
    "                            torch.nn.Linear(hidden_dim*2, action_dim)\n",
    "                    )\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "    def update(self, state, y):\n",
    "            \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "            y_pred = self.model(torch.Tensor(state))\n",
    "            loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def predict(self, state):\n",
    "            \"\"\" Compute Q values for all actions using the DeepQNetwork \"\"\"\n",
    "            with torch.no_grad():\n",
    "                return self.model(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1965ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our deep q network\n",
    "deep_q_network = DeepQNetwork(state_space_size, action_space_size, hidden_dim, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3fbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-LEARNING\n",
    "\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = env.reset()\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > exploration_rate):\n",
    "            q_values = deep_q_network.predict(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update our Q Network due to the reward we got\n",
    "        q_values = deep_q_network.predict(state).tolist()\n",
    "        # Update network weights using the last step only\n",
    "        q_values_next = deep_q_network.predict(new_state)\n",
    "        q_values[action] = reward + discount_rate * torch.max(q_values_next).item()\n",
    "        deep_q_network.update(state, q_values)\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            q_values[action] = reward\n",
    "            # Update network weights\n",
    "            deep_q_network.update(state, q_values)\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd995d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_hundred_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/100)\n",
    "count = 100\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_hundred_episodes:\n",
    "    print(count, \": \", str(sum(reward/100)))\n",
    "    count += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        q_values = deep_q_network.predict(state)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1: # check reward from environment for correct display\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent did not reach the goal.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e3145e",
   "metadata": {},
   "source": [
    "***\n",
    "#### IN SHORT: \n",
    "\n",
    "**Beginners:** \n",
    "If possible discretize the state spaces/observation spaces of your environments(if dealing with continous environments) and use simple, straight forward implementations of the Q-Learning algorithm. \n",
    "\n",
    "**Advanced:**\n",
    "Use a neural network to approximate the q-table of the continous environment (=learning a q-function) and implement the Q-Learning algorithm by being consistent to the environment and agent interfaces of the openai gym framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ea4fb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7afdc3b",
   "metadata": {},
   "source": [
    "# TODO's\n",
    "\n",
    "1. Send your finished presentations (+ possibly annotated paper) by **Monday 12.00 AM/midnight** via email to henrik.voigt@uni-jena.de\n",
    "\n",
    "2. Send your little HOMEWORK to henrik.voigt@uni-jena.de by using the naming convention: HOMEWORK_02_FIRSTNAME_LASTNAME.ipynb until **June 23th 12.00 AM/midnight**\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env-kernel",
   "language": "python",
   "name": "pytorch-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
