{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb14088",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 08 Reinforcement Learning in der Sprachtechnologie\n",
    "## Script-Based Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954dc2ff",
   "metadata": {},
   "source": [
    "### Announcements\n",
    "\n",
    "#### Papers\n",
    "\n",
    "* Yannic Kilcher recently reviewed an interesting paper about arguments why RL might be the only thing we need for creating AGI\n",
    "\n",
    "* you can find the video here: https://www.youtube.com/watch?v=dmH1ZpcROMk\n",
    "\n",
    "* you can find the paper here: https://www.sciencedirect.com/science/article/pii/S0004370221000862\n",
    "\n",
    "#### Homework\n",
    "\n",
    "* Wrong information in last weeks notebook (sry for that ...)\n",
    "        \n",
    "* DEADLINE: JUNE 16th\n",
    "\n",
    "#### Today\n",
    "\n",
    "* short session about script based agents\n",
    "\n",
    "* libraries to check out FYI\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a4e69",
   "metadata": {},
   "source": [
    "### A) Paper Presentation I: Jovanka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4587435e",
   "metadata": {},
   "source": [
    "### **Title: Reinforcement learning of minimalist grammars**\n",
    "**Link:** [MyPaper](https://arxiv.org/pdf/2005.00359v1.pdf)\n",
    "\n",
    "**Summary**\n",
    "\n",
    "The authors provide a machine learning algorithm for a cognitive agent with semantic understanding. The algorithm combines methods from computational linguistics, formal logic and abstract algebra. The syntax is based on minimalist grammar, the semantic on predicate logic. The goal of the agent is to learn the mental lexicon that includes expert language knowledge. This acquisition process is due to a feedback loop applicable for reinforcement learning.\n",
    "\n",
    "    \n",
    "**Problem**\n",
    "\n",
    "* state-of-the-art semantic analysis of user inputs is based on slot-filling procedures\n",
    "  * detect relevant keywords and insert them into semantic frames\n",
    "  * no representation of meaning\n",
    "  * no cognitive understanding of utterances\n",
    "  * goal: overcome traditional slot-filling by proper cognitive information and communication technologies\n",
    "\n",
    "\n",
    "* demand on cognitive user interfaces\n",
    "  * processing and understanding of declarative or imperative sentences\n",
    "    1. syntactic analysis\n",
    "    2. semantic analysis --> semantic representation\n",
    "    3. compute logical inferences\n",
    "    4. respond accordingly (including generation of utterances as feedback signal)\n",
    "    \n",
    "*--> need of language acquisition algorithm*\n",
    "\n",
    "**Idea/Approach**\n",
    "\n",
    "* machine learning algorithm for the acquisition of a minimalist grammar (MG) mental lexicon of the syntax and semantics for English declarative sentences through reinforcement learning\n",
    "* simultaneous segmentation of syntax and semantics\n",
    "* the approach combines methods from computational linguistics, formal logic, and abstract algebra\n",
    "\n",
    "*minimalist grammar (MG):*\n",
    "* important property: effective learnability in the sense of Gold's learning theory\n",
    "* developed by Stabler: mathematical codification of Chomsky's Minimalist Program in the generative grammar framework\n",
    "* consists of\n",
    "  * mental lexicon storing linguistic signs as arrays of syntactic, phonetic and semantic features\n",
    "    * linguistic base types: noun, verb, adjcetive etc.\n",
    "    * selector categories\n",
    "    * licensors and licensees \n",
    "  * two structure-buildung functions: \"merge\" and \"move\"\n",
    "* all syntactic information is encoded in the feature array of the mental lexicon\n",
    "* syntax and compositional semantics can be combined via the lambda calculus\n",
    "\n",
    "**Definitions**\n",
    "\n",
    "* *utterance meaning pairs (UMP)* \n",
    "  * $u = \\langle e,\\sigma\\rangle$, where $e \\in E$ is the spoken or written utterance, given as the *exponent* of a linguistic sign and $\\sigma \\in \\Sigma$ is the sign's *semantics* as a logical term, expressed by means of predicate logic\n",
    "\n",
    "    example: $ u = \\langle \\texttt{the mouse eats cheese, eat} (\\texttt{cheese})(\\texttt{mouse})\\rangle$\n",
    "    \n",
    "\n",
    "* *linguistic signs* are ordered triple\n",
    "  * $z = \\langle e,t,\\sigma \\rangle$, where additionally $t\\in T$ is a *syntactic type* and is encoded by means of MG in its chain representation\n",
    "  * syntactic types control the generation of syntactic structure and the order of lambda application\n",
    "    example: $u = \\langle \\texttt{the mouse eats cheese, :c, eat} (\\texttt{cheese})(\\texttt{mouse})\\rangle$\n",
    "    \n",
    "\n",
    "  * $\\texttt{:c}$ indicates that the sign is complex and a complementizer phrase of type $\\texttt{c}$\n",
    "    \n",
    "* The compositional semantic can be described by the terms $\\lambda\\texttt{P}.\\lambda\\texttt{Q}.\\texttt{P}(\\texttt{Q})$ and $\\lambda\\texttt{P}.\\lambda\\texttt{Q}.\\texttt{Q}(\\texttt{P})$, the predicate $\\texttt{eat}$ and the individuals $\\texttt{cheese}$ and $\\texttt{mouse}$.\n",
    "  * $\\lambda\\texttt{P}.\\lambda\\texttt{Q}.\\texttt{P}(\\texttt{Q})(\\texttt{eat})(\\texttt{cheese})$\n",
    "  * $\\lambda\\texttt{Q}.\\texttt{eat}(\\texttt{Q})(\\texttt{cheese})$\n",
    "  * $\\texttt{eat}(\\texttt{cheese})$\n",
    "  * $\\lambda\\texttt{P}.\\lambda\\texttt{Q}.\\texttt{Q}(\\texttt{P})(\\texttt{mouse})(\\texttt{eat}(\\texttt{cheese}))$\n",
    "  * $\\lambda\\texttt{Q}.\\texttt{Q}(\\texttt{mouse})(\\texttt{eat}(\\texttt{cheese}))$\n",
    "  * $\\texttt{eat}(\\texttt{cheese})(\\texttt{mouse})$\n",
    "\n",
    "\n",
    "* Given the logical term $\\texttt{eat}(\\texttt{cheese})(\\texttt{mouse})$ you can get $\\lambda\\texttt{x}.\\lambda\\texttt{y}.\\texttt{eat}(\\texttt{x})(\\texttt{y})$ with two successive lambda abstraction. So we have both directions: from general to special and from special to general.\n",
    "\n",
    "\n",
    "* *syntactic features*\n",
    "    \n",
    "  * *basic types* $b\\in B = \\{ \\texttt{n,a,v,d,...} \\}$\n",
    "  * *selectors* $S = \\{=b|b \\in B \\}$, unified by *merge*\n",
    "  * *licensors* $L_+ = \\{+l|l \\in L\\}$, where $L$ is a finite set of movement identifiers\n",
    "  * *licensees* $L_- = \\{-l|l \\in L\\}$, licensors and licensees trigger move\n",
    "  * *feature set* $F = B \\cup S \\cup L_+ \\cup L_-$\n",
    "  * *categories* $C=\\{::, :\\}$, where \"::\" indicates *simple, lexical* categories and \":\" *complex, derived* categories\n",
    "  * the ordering of syntactic features is prescribed in the lexicon as regular expressions i.e. $T=C(S\\cup L_+)^*BL_-^*$\n",
    "    \n",
    "example: \n",
    "* $\\langle\\texttt{mouse, ::n, mouse} \\rangle$\n",
    "* $\\langle\\texttt{cheese, ::n -k, cheese} \\rangle$\n",
    "* $\\langle\\texttt{the, ::-n d -k, } \\epsilon \\rangle$   \n",
    "* $\\langle\\texttt{eat, ::-n v -f, }\\lambda\\texttt{x}.\\lambda\\texttt{y}.\\texttt{eat}(\\texttt{x})(\\texttt{y}) \\rangle$    \n",
    "* $\\langle\\texttt{-s, ::-pred +f +k t, }\\epsilon\\rangle$\n",
    "* $\\langle\\epsilon\\texttt{, ::-v +k -d pred, }\\lambda\\texttt{P}.\\lambda\\texttt{Q}.\\texttt{eat}(\\texttt{Q})(\\texttt{P})\\rangle$  \n",
    "* $\\langle\\epsilon\\texttt{, ::-t c, }\\epsilon \\rangle$\n",
    "\n",
    "    \n",
    "* syntactic functions (p. 7)\n",
    "merge-1, merge-2, merge-3 and move-1, move 2\n",
    "    \n",
    "A minimalist derivation terminates when all syntactic features besides one distinguished *start symbol*, which is $\\texttt{c}$ (complementizer phrase) in our case,  have been consumed.\n",
    "    \n",
    "example for derivation (bottom-up) (p. 10)\n",
    "\n",
    "**Utterance-Meaning Transducer (UMT)**\n",
    "    \n",
    "* bottom-up derivations are essential for MG\n",
    "* but they are not suitable for NLP, because their computation is neither incremental nor predictive\n",
    "* therefore the authors present a bidirectional utterance-meaning transducer for MG\n",
    "* central object for MG language processing is derivation tree\n",
    "* derivation tree with comma-separated sequence of exponents and index tuples for every node (p. 12)\n",
    "* derivation = tree paths from the bottom to the top\n",
    "* tree paths from the top towards the bottom allows for an interpretation in terms of *multiple context-free grammars* (MCFG)\n",
    "* MCFG: categories are *n*-ary predicates over string exponents\n",
    "* every branching leads to one phrase structure rule in the MCFG (p. 13)\n",
    "* reversion of MG rules\n",
    "* UMT with language production module and language understanding module (details pp. 13)\n",
    "  * language production: get semantic representation and compute the exponent\n",
    "  * language understanding: three memory tapes: input sequence, syntectic priority queue, semantic priority queue\n",
    "    \n",
    "**Reinforcement Learning**\n",
    "* RL algorithm for learning the MG lexicon\n",
    "* training algorithm that simultaneously analyzes similarities between exponents and semantic terms\n",
    "* positive and negative examples to obtain a better performance through reinforcement learning\n",
    "* model:    \n",
    "  * cognitive agent $L$ in a state $X_t$, identified as $L$'s mental lexicon at training time $t$\n",
    "  * for $t=0$ $X_0$ is initialized as $X_0 \\leftarrow \\emptyset$\n",
    "  * $L$ is exposed to UMPs produced by a teacher $T$\n",
    "  * assumption: $T$ presents already complete UMPs, not singular utterances to $L$ \n",
    "    * avoid *symbol grounding problem* of firstly assigning meaning $\\sigma$ to uttered exponents $e$\n",
    "  * $L$ is instructed to reproduce $T$'s utterances based on its own semantic understanding\n",
    "    * provides feedback loop --> applicable for reinforcement learning\n",
    "  * in each iteration, the teacher utters an UMP that should be learned by the learner\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/2021_05_29 19_54 Office Lens.jpg\" width=\"500\"/>\n",
    "\n",
    "example\n",
    "\n",
    "$t = 1$:\n",
    "\n",
    "$u_1 = \\langle \\texttt{the mouse eats cheese, eat}(\\texttt{cheese})(\\texttt{mouse})\\rangle$\n",
    "* as long as $L$ is not able to detect patterns or common similarities in $T$'s UMPs, it simply adds new entries directly to its mental lexicon, assuming the UMP is comlex \":\", possessing base type $\\texttt{c}$\n",
    "* update rule for $L$'s mental lexicon: $X_t \\leftarrow X_{t-1} \\cup \\{\\langle e_t, \\texttt{:c}, \\sigma_t\\rangle \\}$, when $u_t = \\langle e_t, \\sigma_t \\rangle$ is the UMP presented at time $t$ by $T$\n",
    "* mental lexicon $X_1 = \\{\\langle \\texttt{the mouse eats cheese, :c, eat}(\\texttt{cheese})(\\texttt{mouse})\\rangle \\}$\n",
    "\n",
    "$t = 2$:\n",
    "\n",
    "$u_2 = \\langle \\texttt{the rat eats cheese, eat}(\\texttt{cheese})(\\texttt{rat})\\rangle$\n",
    "* similarities between $u_1$ and $u_2$\n",
    "* $L$ creates to distinct items for $\\texttt{the mouse}$ and $\\texttt{the rat}$\n",
    "* $L$ carries out lambda abstraction to obtain the updatet lexicon $X_2$\n",
    "* $X_2 = \\{\\langle \\texttt{the mouse, :d, mouse}\\rangle, \\langle \\texttt{the rat, :d, rat}\\rangle, \\langle \\texttt{eats cheese, :-d c, } \\lambda y.\\texttt{eat}(\\texttt{cheese})(y)\\rangle \\}$\n",
    "* further segmentation\n",
    "* $X_{21} = \\{\\langle \\texttt{the, ::-n d, }\\epsilon \\rangle, \\langle \\texttt{mouse, ::n, mouse}\\rangle, \\langle \\texttt{rat, ::n, rat}\\rangle, \\langle \\texttt{eats cheese, :-d c, } \\lambda y.\\texttt{eat}(\\texttt{cheese})(y)\\rangle \\}$\n",
    "* for closing of the reinforcement cycle, $L$ is supposed to produce utterances on its own understanding\n",
    "* assumption $L$ wants to express the proposition $\\texttt{eat}(\\texttt{cheese})(\\texttt{rat})$\n",
    "* correct derivation with the corresponding signs from the lexicon is made leading to correct utterance $\\texttt{the rat eats cheese}$\n",
    "* $T$ endorses this utterance\n",
    "\n",
    "\n",
    "$t=3$:\n",
    "\n",
    "$u_3 = \\langle \\texttt{the mouse eats carrot, eat}(\\texttt{carrot})(\\texttt{mouse})\\rangle$\n",
    "\n",
    "* similarities with lexicon entry $ \\langle \\texttt{eats cheese, :-d c, } \\lambda y.\\texttt{eat}(\\texttt{cheese})(y)\\rangle$\n",
    "* lambda abstraction\n",
    "* $X_{3} = \\{\\langle \\texttt{the, ::-n d, }\\epsilon \\rangle, \\langle \\texttt{mouse, ::n, mouse}\\rangle, \\langle \\texttt{rat, ::n, rat}\\rangle, \\langle \\texttt{cheese, ::n, cheese}\\rangle, \\langle \\texttt{carrot, ::n, carrot}\\rangle, \\langle \\texttt{eats, ::-n -d c, } \\lambda x.\\lambda y.\\texttt{eat}(x)(y)\\rangle \\}$\n",
    "* $L$ produces utterance of novel semantic represantation $\\texttt{eat}(\\texttt{carrot})(\\texttt{rat})$\n",
    "* derivation of UMP $\\langle \\texttt{the rat eats carrot}, \\texttt{eat}(\\texttt{carrot})(\\texttt{rat})\\rangle$\n",
    "* UMP is rewarded by $T$\n",
    "\n",
    "$t=4$:\n",
    "\n",
    "$u_4 = \\langle \\texttt{the rats eat cheese, eat}(\\texttt{cheese})(\\texttt{rats})\\rangle$\n",
    "\n",
    "* pattern matching\n",
    "* $X_{4} = \\{\\langle \\texttt{the, ::-n d, }\\epsilon \\rangle, \\langle \\texttt{mouse, ::n, mouse}\\rangle, \\langle \\texttt{rat, ::n, rat}\\rangle, \\langle \\texttt{rats, ::n, rats}\\rangle, \\langle \\texttt{cheese, ::n, cheese}\\rangle, \\langle \\texttt{carrot, ::n, carrot}\\rangle, \\langle \\texttt{eats, ::-n -d c, } \\lambda x.\\lambda y.\\texttt{eat}(x)(y)\\rangle, \\langle \\texttt{eat, ::-n -d c, } \\lambda x.\\lambda y.\\texttt{eat}(x)(y)\\rangle \\}$\n",
    "* new proposition $\\texttt{eat}(\\texttt{carrot})(\\texttt{rats})$\n",
    "* derivation leads to UMP $\\texttt{the rats eats carrot}$\n",
    "* $T$ rejects utterance because of grammatical number agreement error\n",
    "* $L$ has to find suitable revision of $X_4$\n",
    "* new features for number: $\\texttt{a}$ and $\\texttt{num}$\n",
    "* new entry: plural suffix\n",
    "* $X_{41} = \\{\\langle \\texttt{the, ::-num d, }\\epsilon \\rangle, \\langle \\texttt{mouse, ::n -a, mouse}\\rangle, \\langle \\texttt{rat, ::n -a, rat}\\rangle, \\langle \\texttt{-s, ::-n +a num, }\\epsilon \\rangle, \\langle \\texttt{cheese, ::n, cheese}\\rangle, \\langle \\texttt{carrot, ::n, carrot}\\rangle, \\langle \\texttt{eats, ::-n -d c, } \\lambda x.\\lambda y.\\texttt{eat}(x)(y)\\rangle, \\langle \\texttt{eat, ::-n -d c, } \\lambda x.\\lambda y.\\texttt{eat}(x)(y)\\rangle \\}$\n",
    "\n",
    "and so on\n",
    "\n",
    "**Critique**\n",
    "* theoretical and abstract paper\n",
    "* explanation of approach with examples\n",
    "* contribution to an important problem: computational semantic represantation of utterances\n",
    "* language acquisition algorithm provides understanding and generation of utterances\n",
    "* many points of contact to other concepts and problems: minimalist grammar, multiple context-free grammars, Gold's learning theory, symbol grounding problem, predication logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c29cd4",
   "metadata": {},
   "source": [
    "### A) Paper Presentation II: Toni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14490016",
   "metadata": {},
   "source": [
    "### Title: Reinforcement Learning for Relation Classification from Noisy Data\n",
    "\n",
    "#### Link:\n",
    "[Reinforcement Learning for Relation Classification from Noisy Data](https://arxiv.org/pdf/1808.08013.pdf)\n",
    "\n",
    "#### Summary:\n",
    "Existing relation classification methods that rely on distant supervision assume that \n",
    "a bag of sentences mentioning an entity pair are all describing a relation for the entity pair. \n",
    "Such methods, performing classification at the bag level, cannot identify the mapping between \n",
    "a relation and a sentence, and largely suffers from the noisy labeling problem. In this paper,\n",
    "we propose a novel model for relation classification at the sentence level from noisy data. \n",
    "\n",
    "\n",
    "#### Problem/Task/Question:\n",
    " - Can we use reinforcement learning methods to classificate noisy relations of 2 entities?\n",
    " - NLP Problem:\n",
    "     - categorize semantic relations between 2 entities given a plain text  \n",
    "    \n",
    " -  Noisy labeling problem:\n",
    "     - (Barack_Obama, BornIn, United_States) = (entity,relation,entity) tripel\n",
    "     - \"Barack Obama is the 44th president of the United States\" will be regarded as positive instance\n",
    "        by distant supervision for relation BornIn\n",
    "     - (53% of 100 sample bags have no sentences that describes the relation of the entities)\n",
    "        \n",
    " - previous distant super vision methods suffer from noisy labeling problem, because they assume that 2 entities are in a relation if this relation is mentioned in the sentences\n",
    " \n",
    " - instance selection problem, which sentence truly describes relation and should be selected as training instance?\n",
    " - relation classification problem, which semantic relation has the highest probability in a sentence and mentioned entity pair?\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/sentencebaglevel.PNG \"\")\n",
    "- bag level: -  bag contains noisy sentences with same entities (possibly not same relation)\n",
    "- 2 problems in bag level approach:\n",
    "    - 1. unable to handle sentence level prediction\n",
    "    - 2. sensitive to bags with all noisy sentences (no relation described)\n",
    "    - decreases performance of relation classification\n",
    "\n",
    "#### Solution/Idea/Model/Approach:\n",
    "\n",
    "\n",
    "**Overall Structure:** \n",
    "- instance selector chooses sentences according to policy function\n",
    "- the selected sentences are used to train a better relation classifier\n",
    "- instance selector updates its parameters, with reward from relation classifier\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/overallstructure.PNG \"\")\n",
    "\n",
    "\n",
    "\n",
    "This model consists of 2 modules:\n",
    "- 1. instance selector: \t\n",
    "   - acts as agent\n",
    "   - first select high quality sentences from a sentence bag\n",
    "   - has no explicit knowledge about correct labeled sentences\n",
    "   - reinforcement learning, we can measure the utility of the selected sentences as whole\n",
    "   - filter entire bag if all sentences are labeled incorrectly\n",
    "      \n",
    "- 2. relation classifier: \n",
    "     - predict relation from each sentence in cleansed data and sentence level p(r|x)\n",
    "     - provide reward to instance selector\n",
    "\n",
    "Training data: \n",
    "- text data\n",
    "- widely used dataset NYT:\n",
    "     - 522k sentences, 281k entity pairs, 18k relational facts + NA that says: \"there is no relation\"\n",
    "- word2vec to train word embedding on NYT Corpus (relative distances from the current word) (word and position embedding)\n",
    "- Convolutional Neural Network (CNN) as relation classifier\n",
    "- three-fold cross validation for tuning the model\n",
    "\n",
    "\n",
    "Training: \n",
    "- we want to: \n",
    "    - optimize policy network in instance selector with policy gradient method\n",
    "    - optimize the CNN Component they use gradient descent method to minimize the loss function (cross entropy)\n",
    "\n",
    "- first pretrain CNN, then pretrain policy function by computing reward with the pretrained CNN (with frozen parameters)\n",
    "- then jointly train the instance selector and relation classifier\n",
    "- when training of instance manager is done, we merge all selected sentences in each bag to obtain a cleansed dataset and train the relation classifier with it\n",
    "- supervise instance selector to maximize the average likelihood of choosen instances\n",
    "\n",
    "\n",
    "**reinforcement learning spaces:**\n",
    "\n",
    "**state:**\n",
    " - current sentence, already selected sentence and entity pair when\n",
    "   making descision on the i-th sentence of bag B\n",
    " - as continuous real-valued vector\n",
    "\n",
    "**action:**\n",
    " - action space: {0,1}\n",
    " - instance selector selects as training instance the i-th sentence of bag B (1) or not (0)\n",
    " - selects according to stochastic policy\n",
    " \n",
    "**policy:**\n",
    " - normally updated after selection on all training instances are finished (with policy gradient theorem and REINFORCE algorithm)\n",
    " - but, we can split the training sentence instances into N Bags, and compute the reward for each finished bag\n",
    " - each bag corresponds to a distinct entity pair, and has a sequence of sentence with the same noisy relation label\n",
    "\n",
    "**reward**\n",
    " - indicates the utility of the chosen sentences\n",
    " - relation classifier provides reward to instance selector\n",
    " - we only receive delayed reward after last sentence of the bag, zero in all other states\n",
    " - if bag empty, reward is the average likelihood of all sentences in the training data (to exclude noisy bag)\n",
    " - we aim to maximize the expected total reward\n",
    " \n",
    "**transition**\n",
    " - load next sentence of bag and choose an action\n",
    "\n",
    "\n",
    "\n",
    "#### Results:\n",
    "  ![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/performance.PNG \"\")\n",
    " - performance on sentence level classification\n",
    " - (F1 metric for classification problems, recall and precision)\n",
    " - CNN+RL has the best accuracy with 64%\n",
    " - previous methods could not filter the bags with noisy sentences\n",
    " - instance selector can exlude noisy sentences effectively (they selected 100 deleted sentence bags\n",
    "   and found that 86% of the bag consist of all noisy sentences)\n",
    "\n",
    "\n",
    "**Different Models**\n",
    "\n",
    " - CNN(2014) sentence-level classification model, doesnt consider noisy labeling problem\n",
    " - CNN+Max(2015) bag-level classification, assumes there is one sentence in the bag, thats describing the relation (most correct sentence in each bag)\n",
    " - CNN+ATT(2016) bag-level model, similar to CNN+Max, can weight down noisy sentences. because it addopts sentence level attention)\n",
    "\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    " - selected data by instance selector is better for relation classification\n",
    " ![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/precision.PNG \"\")\n",
    " - with weighting down noisy sentences\n",
    " ![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/precisionwithweights.PNG \"\")\n",
    " - accuracy of selection decision\n",
    " \n",
    " - compared reinforcement learning selection with greedy selection\n",
    " - greedy selection selects top N sentences with largest likelihood (estimated by a pre trained CNN)\n",
    " ![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/greedy.PNG \"\")\n",
    "\n",
    "\n",
    "\n",
    "#### Critical Discussion:\n",
    "\n",
    "* **+** deals better with noisy data, unlike reducing weights \n",
    "* **+** classification at sentence level (relation classifier trained and tested at sentence level on cleansed data)\n",
    "* **+** good structure and visualisations of the results\n",
    "\n",
    "\n",
    "* **-** instance selector rejects more than just the noisy instances\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ad6dd",
   "metadata": {},
   "source": [
    "### A) Paper Presentation III: Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0ff31",
   "metadata": {},
   "source": [
    "### Reinforced Extractive Summarization with Question-Focused Rewards\n",
    "##### Link:\n",
    "[A Survey on Reinforcement Learning for Dialogue Systems](https://arxiv.org/pdf/1805.10392.pdf)\n",
    "\n",
    "##### Summary:\n",
    "This Paper aims at derriving extractive summaries, which are summaries that are made up of a subset of word sequences in the source document. As oppose to abstractive summaries, this approach to summarization is less pron to diverging from the facts in the document. They extract 'questions' from human written summaries to evaluate and reinforce automatically generated summareis that retain the necessary information. This way, they use reinforcemnet learning to explore the space of possible summareis.\n",
    "\n",
    "##### Problem/Task/Question:\n",
    "* Automatically extracting word sequnces from a source document such the resulting text contains the same information as the human written summaries\n",
    "* human abstracts traditionally do not align on align on word level but used as ground truth labels for extractive summarization anyway\n",
    "\n",
    "##### Solution/Idea/Model/Approach:\n",
    "* **Design a question orientated reward function**\n",
    "    * Extract \"Cloze questions\" $Q_k$ from each sentence in human written summary by replacing **entities** or **root words**, i.e. root of sentence dependency parse tree, with placeholder token.\n",
    "    * Encode entire question sequence Bi-LSTM$(Q_k) =q_k$\n",
    "    * Given extractive summary $Y$ use Bi-LSTM to ecode every word at postion i in the summary $h_i^Y$\n",
    "    * Use attention mechanism to locate which part of summary is relavent to question $\\alpha_{k,i} \\propto exp(q_k W^{\\alpha}h_t^Y)$\n",
    "    * For every question, compute a context vector $c_k$ as $\\alpha$-weighted sum of word encodings $h_i^Y$ to predict correct word for placeholder P$(e_k|Y,Q_k)=$ softmax$(W^c c_k)$\n",
    "    * The reward is defined as log-likelihood of correctly predicting token averaged over all questions $R_a(Y) = \\frac{1}{K} \\sum_{k=1}^{K}logP(e_k|Y,Q_k)$\n",
    "    * Other reward components: \n",
    "    - $R_s(Y)$ consciese by restricting number of words\n",
    "    - $R_f(Y)$ fluency by encouraging consequtive words in source documnt to be selected \n",
    "    - $R_b(Y)$ encourages original wording by overlapping of biagrams\n",
    "    * final reward is weighted sum of reward components \n",
    "$R(Y) = R_a(Y)+\\gamma R_b(Y)+\\beta R_f(Y)+\\alpha R_s(Y)$\n",
    "\n",
    "![img](https://d3i71xaburhd42.cloudfront.net/f383d0a898df9f59538a8bdeff9b44bd9055c8af/3-Figure1-1.png)\n",
    "\n",
    "* **Extract summareis with Reinforcement Learning**\n",
    "    * Seek policy $P(Y|X)$ to extract summary $Y$ for any source document $X$ such that $\\mathbb{E}_{P(Y|X)}[R(Y)]$ is maximized\n",
    "    * Models used for extracting summaries:\n",
    "        * Bi-LSTM$(X)$ used to encode each word in the document ($h_i^X$)\n",
    "        * LSTM where hidden state encodes previous sampling decisions $s_{t-1}$\n",
    "        * => as concatenated input for feedforward layer with sigmoid activation to retrieve sampling decisions \n",
    "    \n",
    "* **Training and Hyperparameter tuning**\n",
    "    * First models used for extracting reward response are trained using a subset of the CNN dataset (articles and summaries)\n",
    "    * CNN dataset used to pretrain the Bi-LSTM$(X)$ and LSTM for sampling decisions\n",
    "    * Sample multiple summaries $\\hat{Y}$ by introducing dropout rate in the extraction process\n",
    "    * Use reward for all gnerated summaries to weight the gradients used to update the two models and feedforward layer \n",
    "    * Hyperparameters such as $\\alpha$, $\\gamma$, $\\beta$, hidden stat size of model, drop out rate and summary length tuned. with validation set\n",
    "    \n",
    "##### 3 Main Results/Findings:\n",
    "* This approach outperforms all methods they compare to when using the ROUGE-Metric for evaluation\n",
    "* There is no significant difference between using one question or using five questions, possibly, because articles are ery short \n",
    "* removing sentence root words for questions is the more viable strategy\n",
    "\n",
    "![img](https://d3i71xaburhd42.cloudfront.net/f383d0a898df9f59538a8bdeff9b44bd9055c8af/5-Table3-1.png)\n",
    "\n",
    "![img](https://d3i71xaburhd42.cloudfront.net/f383d0a898df9f59538a8bdeff9b44bd9055c8af/4-Table2-1.png)\n",
    "\n",
    "![img](https://d3i71xaburhd42.cloudfront.net/f383d0a898df9f59538a8bdeff9b44bd9055c8af/6-Table4-1.png)\n",
    "\n",
    "##### Critical Discussion:\n",
    "* -training process not explained at all\n",
    "* +clear description of reward function\n",
    "* -some issues with indexing\n",
    "* -question answering accuracy seems low (?)\n",
    "* -only one (not very convincing) qualitative result, without detailing how it was selected \n",
    "* +future work: qualtivative analysis\n",
    "* +comparing to many recent other works\n",
    "* -cherry picked comparison works. In 2018, when this was published, better Rouge metrics had been achieved. So claim made in abstract 'surpassing state-of-the' is not true (https://paperswithcode.com/sota/extractive-document-summarization-on-cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4930851",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929da45",
   "metadata": {},
   "source": [
    "## B) Theory\n",
    "\n",
    "### RL in LANGUAGE TASKS | NLP\n",
    "\n",
    "> LANGUAGE GAMES being able to be modeled by RL: \n",
    "\n",
    "> Translation Agent: see sentence -> action space vocab -> reward based on the appropriateness of the translation. \n",
    "\n",
    "> Language Learning: environment with sounds, words, images -> reward from teacher?\n",
    "\n",
    "> Google Assistant/Personal Assistant: user asks question -> assistant gives answer -> QA-Dialogue!!!\n",
    "\n",
    "> Information Retrieval: explanation creation -> SCORING => adaptive explanations/generations\n",
    "\n",
    "> Human Language Learning: producing sounds, BIAS(no,..), GOALS (achieve common ground, understanding, does he/she understand me?), \n",
    "\n",
    "> FUTURE SESSION: language immergence, how is language immerging in an environment -> why and how do agents exchange sounds to ACHIEVE THINGS/GOALS?!!! (deepmind group)\n",
    "\n",
    "#### ENVIRONMENT: \n",
    "* STATE_SPACE: different possibilities to model this, one suggestion: DIALOGUE of fixed length N -> all possible sequences consisting of utterances of max_len L with max_seq_len N \n",
    "* e.g. Chat back and forth:\n",
    "\n",
    "![alt text](https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/e8b92e3a-74f3-4403-a609-d401592c5919/topbots-chatbots-2-mitsuku-goodconversationalist-opt.png \"\")\n",
    "\n",
    "* STATE: current utterance (1...N) \n",
    "\n",
    "* continous state space = COMMON GROUND of the conversation of all particpating agents => cross product of the agents state graphs, agent state graph = current utterance + memory of N utterances + future n step lookahead\n",
    "\n",
    "![alt text](https://lh3.googleusercontent.com/proxy/5BB_NDe4kkUgNLmxo2cD5qB37qRJkGB2XVNQtJFVcpBhfs8TcYj5iKvAmdKHXhBhI4Bujyb7Vw \"\")\n",
    "\n",
    "* STATE: continous state space common mind state/common ground of dialogue -> common ground dialogue state (memory, now, future graph cross product)\n",
    "\n",
    "![alt text](https://www.neoformix.com/2007/StateOfUnion_2007.png \"\")\n",
    "\n",
    "* OBSERVATION_SPACE: categorical: dialogue sequence, continous: memory of dialogue utterances, current utterance, own lookahead\n",
    "\n",
    "* OBSERVATION: categorical: single utterance, continous: state graph\n",
    "\n",
    "#### AGENT: \n",
    "* ACTION SPACE: [VOCABULARY]\n",
    "\n",
    "![alt text](http://colah.github.io/posts/2015-01-Visualizing-Representations/img/wiki-pic-major.png \"\")\n",
    "\n",
    "* ACTION: [WORD or SENTENCE]\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/91KjB.png \"\")\n",
    "\n",
    "\n",
    "* REWARD FUNCTION: different possibilities to model this: \n",
    "* a) right, wrong (QA) or yes, no, true, false, (0,1) up to probabilities of agreement, disagreement (0...1)\n",
    "* b) sentiment (+1 pos,-1 neg) up to probabilities of positivity, negativity, neutrality (0...1)\n",
    "* c) praise and blame (you did this very well, because ...., this is not very good, because) (0..1)\n",
    "* d) common sense, common ground, understanding, reflection => related to theory of mind => how much does my opponent understand me? can he follow me? are we on the same mental ground?\n",
    "* e) multimodal feedback -> face expressions, smiling, emotions\n",
    "* f) ...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbcf189",
   "metadata": {},
   "source": [
    "## C) Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa211c6e",
   "metadata": {},
   "source": [
    " **Script based conversational agents**\n",
    " \n",
    " NLTK provides simple, pattern based chatbot functionality. \n",
    " \n",
    " It  can be used for QA-Chatbots or Chatbots that retrieve specific answers out of document corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1113b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from nltk.chat.util import Chat, reflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b3f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN // Hard coded // scripted responses // pattern matching\n",
    "pairs =[\n",
    "    [r\"my name is (.*)\", ['Hello %1!']],\n",
    "    ['(hi|hello|hey|holla|hola)', ['Hey there !', 'Hi there !', 'Hey !']],\n",
    "    ['(.*) your name ?', ['My name is Geeky']],\n",
    "    ['(.*) do you do ?', ['We provide a platform for tech enthusiasts, a wide range of options !']],\n",
    "    ['(.*) created you ?', ['Geeksforgeeks created me using python and NLTK']], \n",
    "    ['(.*) need help', ['how can i help you?']],\n",
    "]\n",
    "\n",
    "# Usage examples:\n",
    "# used for QA -> e.g. find similar phrases in a corpus using TFIDF and using these as answers\n",
    "# Covid Vaccine Notifier Bots\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74276de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> whats your name ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Geeky\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> what do you do?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We provide a platform for tech enthusiasts, a wide range of options !\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> how do you do?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We provide a platform for tech enthusiasts, a wide range of options !\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "chat = Chat(pairs, reflections)\n",
    "chat.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb0d83",
   "metadata": {},
   "source": [
    "#### Rasa Chatbots\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2400/1*Bs0JvC6bmiwrC7we49-tjw.png\" width=\"500\"/>\n",
    "\n",
    "Script based conversational agents can be build using RASA. \n",
    "\n",
    "Rasa extends the pattern matching approach to **DIALOGUE FLOWS** which allow guided dialogues. \n",
    "\n",
    "Rasa furthermore uses pre-trained ML models for better language understanding. \n",
    "\n",
    "Find the package here:\n",
    "\n",
    "https://rasa.com/\n",
    "\n",
    "https://github.com/RasaHQ/rasa\n",
    "\n",
    "You can install it this way: \n",
    "\n",
    "`pip install rasa`\n",
    "\n",
    "`pip install spacy`\n",
    "\n",
    "`python -m spacy download en`\n",
    "\n",
    "`pip install nest_asyncio==1.3.3`\n",
    "\n",
    "\n",
    "You can find a detailed explanation using **google colab** here: \n",
    "\n",
    "https://www.youtube.com/watch?v=G9Z6NQ3EcEw&list=PLnmxk2xwn3zHL9NwYKV2_7vJUXw8YVhZA\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b81f7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6692533",
   "metadata": {},
   "source": [
    "# TODO's\n",
    "\n",
    "1. Send your finished presentations (+ possibly annotated paper) by **Monday 12.00 AM/midnight** via email to henrik.voigt@uni-jena.de\n",
    "\n",
    "2. Send your little HOMEWORK to henrik.voigt@uni-jena.de by using the naming convention: HOMEWORK_02_FIRSTNAME_LASTNAME.ipynb until **June 16th 12.00 AM/midnight**\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7dcab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-env-kernel",
   "language": "python",
   "name": "huggingface-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
