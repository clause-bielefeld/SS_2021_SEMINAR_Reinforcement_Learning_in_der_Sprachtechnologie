{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d757db",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 07 Reinforcement Learning in der Sprachtechnologie\n",
    "## Practice Session II : RL Examples in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412a5d1",
   "metadata": {},
   "source": [
    "### Announcements\n",
    "\n",
    "#### Papers\n",
    "\n",
    "* Awesome work!\n",
    "\n",
    "* Last time we spoke about language aquisitoin and the role of RL in that: What Topics are you interested in related to RL and NLP? Let me know, so we can discuss!\n",
    "\n",
    "#### Homework\n",
    "        \n",
    "* DEADLINE: JUNE 19th\n",
    "\n",
    "#### Future\n",
    "\n",
    "* We are moving towards the practical sessions, that means we will have paper discussions, a very rough topic discussion (5-10 min) and then we will take a look at CODE\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88831c4",
   "metadata": {},
   "source": [
    "### A) Paper Presentation I: Daniel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f927b04",
   "metadata": {},
   "source": [
    "### Title: Sentiment Analysis for Reinforcement Learning\n",
    "\n",
    "#### Link:\n",
    "[Sentiment Analysis for Reinforcement Learning](https://arxiv.org/abs/2010.02316v1)\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "That paper shows the advantage of sentiment analysis in sparse reward enviorments, especially in Text based games.\n",
    "Instead of training straight forward to reach the sparse goal states, the seniment of the enviormental feedback is analyzed, and decisions are made upon this analysis. The authors first state the problem and the common aproaches, show their aproach and show the advantages of their approach in metrics.\n",
    "\n",
    "#### Problem/Task/Question:\n",
    "Can we use sentiment analysis to convert a sparse reward problem into a dense one?\n",
    "\n",
    "For this task the following partial problems need to be addressed:\n",
    "- Sentiment analysis in NLP and RL (generating auxiliary rewards)\n",
    "- Actually training for solving the task\n",
    "\n",
    "\n",
    "#### Solution/Idea/Model/Approach:\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/Model01_Sentimentortrag.png \"\")\n",
    "\n",
    "**Overall Structure:** \n",
    "\n",
    "Text description represents a state:\n",
    "- LSTM is used to encode the state\n",
    "    - LSTM receives the words as input\n",
    "    - produces state repr. as avg. of all LSTM outputs\n",
    "- the encoding is feeded into the DQN\n",
    "    - 2 Layer NN that calculates Q-Function\n",
    "\n",
    "enviorment:\n",
    "- TextWorld learning environment for text-based games\n",
    "    - centered on cooking a recipe, requiring agents to determine the necessary ingredients from a recipe book\n",
    "    - collecting in the house\n",
    "    - overcoming obstacles\n",
    "    - changing obstacles, ingredients, recipes, and environment\n",
    "- 1,048 TextWorld game trajectories (524 wins and 524 losses)\n",
    "\n",
    "Training on auxiliary Data:\n",
    "- Chat from players with game, that does not change state\n",
    "- Hypothesis: this chat ist more sentiment loaded than the rest\n",
    "\n",
    "\n",
    "RL with supplement reward:\n",
    "- The reward, given by the enviorment is supplemented  \n",
    "- This is done by a sentiment-based reward extracted from the state representation\n",
    "- Even if the enviormental reward is 0, it can speed up learning\n",
    "- assumption: access to positive and negative trajecories for sentiment analyzer\n",
    "- experience replay\n",
    "    - successfull trajectories are replayed more often\n",
    "- Using a fine-tuned BERT for learning from the enviorment and the chat\n",
    "- Correlation between mean positive senitment in trajectory and success (0.713)\n",
    "- Banter (text from users to chat) is mostly positive if winning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**reinforcement learning spaces:**\n",
    "\n",
    "**state:**\n",
    " - text of the last feedback + the preceeding states\n",
    "\n",
    "**action:**\n",
    "- action is to choose between different alternatives in textbased games\n",
    "\n",
    "**reward**\n",
    "- of course the main reward\n",
    "- intermediate rewards\n",
    "- auxilliary rewards\n",
    "\n",
    "**transition**\n",
    "- calculate sentiment and choose next action based on that\n",
    "\n",
    "\n",
    "#### Results:\n",
    "\n",
    "- The problem of to high auxilliary rewards, so no motivation is there to explore further\n",
    "    - this is solved by scaling the reward down to 10%\n",
    "- evalutating models in two settings: with \\<Model\\> and without intermediate results \\<Model-Zero\\>\n",
    "\n",
    "**Different Models**\n",
    "- Naive Bayes\n",
    "- BERT (fine-tuned on Stanford movie review dataset, positive and negative trajectories, banter)\n",
    "    - Extra Variant with threshold about polarity (pos or neg) in state, cause they are noisy\n",
    "\n",
    "**Evaluation**\n",
    "- Assumption: agent is interacting in same env. like trained\n",
    "- For fair comparrison: only include rewards received by inviorment\n",
    "- the authors say that the maximum score @ the end matters the most, because model migth reach optimum at the end\n",
    "- Large auxiliary rewards aren't, cause migth steer agent away from goal\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/TrainingResults.png \"\")\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/TrainingResultsTable.png \"\")\n",
    "- auxiliary rewards without intermediate reward are much noisier\n",
    "- intermediate Rewards have direct access to the game\n",
    "- naive Bayes is working on word-level -> no semantic meaning\n",
    "- Banter-BERT oes not perform better as vanilla model (without auxilliary)\n",
    "    - most likely bad generalization\n",
    "- Other Berts Outperform Vanilla Models\n",
    "    - BERT-Traj is best because of high domain-similarity\n",
    "\n",
    "#### Conclusion and annotations\n",
    "- auxiliary rewards can halp improve RL agents in text domains\n",
    "- can maybe be used in task oriented dialogue\n",
    "\n",
    "**Similiarities**\n",
    "- The approach has similarities to intrinsically motivated RL\n",
    "    - IMRL has only intrinsic rewards (observation of enviorment)\n",
    "    - Sentiment engine can be considered as IMRL\n",
    "- similarity to techer student Method\n",
    "    - sentiment analysis is teacher that provides Feedbeck\n",
    "![alt text](https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/IMRL.png \"\")\n",
    "\n",
    "#### Critical Discussion:\n",
    "\n",
    "* **+** Very good Language\n",
    "* **+** Methods are shown well\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **-** The domain in which this can be used is very Limited\n",
    "* **-** The auxiliary rewards are very noisy\n",
    "* **-** The models do not generalize well (as shown)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e880bf5f",
   "metadata": {},
   "source": [
    "### A) Paper Presentation II: Harry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3b408",
   "metadata": {},
   "source": [
    "### Title: Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation\n",
    "\n",
    "### Link: \n",
    "https://arxiv.org/abs/2012.02952\n",
    "\n",
    "### Summary:\n",
    "The paper presents a framework, Data Boost, that can augment text data. It shows how data augmentation was implemented using reinforcement learning. Data Boost shows great results compared to other methods of data augmentation. Furthermore, Data Boost can benefit classifiers.\n",
    "\n",
    "### Problem/Task/Question:\n",
    "Typically, classifiers have low performance when the data is low. The idea to solve the problem is to add modified copies of the existing data for training. <br>\n",
    "The challenge is to generate good data that reflects the original class labels and has as good readability as the original data. Previous methods are not convincing and suffer from poor readability, unsatisfactory semantic consistency and lexical shrinkage\n",
    " \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/example_augmentation.png\" alt=\"example_augmentation\" style=\"zoom:50%;\" />\n",
    "\n",
    "\n",
    "### Solution/Idea/Model/Approach:\n",
    "use of **state-of-the-art language models**\n",
    "+ trained normally on a **large** amount of **data**\n",
    "+ **key idea**: use the resource contained in the language models\n",
    "+ language models will generate the augmented data\n",
    "    + use of **natural language understanding**\n",
    "    + use of **natural language generation**\n",
    "    <br><br>\n",
    "\n",
    "\n",
    "\n",
    "built **off-the-shelf** language model (**vanilla GPT-2**)\n",
    "+ GPT-2 is an unsupervised **large transformer-based** language model trained to generate text by predicting the next word in a sequence of tokens\n",
    "    + trained on a dataset of 8 million web pages\n",
    "+ **no need** for **external datasets or training** separate systems from the scratch\n",
    "+ off-the-shelf GPT-2 language model with modified decoding stage without changed architecture\n",
    "+ convert GPT-2 into a conditional generator \n",
    "    + **guide** the generator towards specific class labels during its decoding stage **through reinforcement learning**\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/LM.png\" alt=\"LM\" style=\"zoom:50%;\" />\n",
    "\n",
    "\n",
    "\n",
    "**Reward Function**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/Reward-Function.png\" alt=\"example_augmentation\" style=\"zoom:65%;\" />\n",
    "\n",
    "\n",
    "+ **state** $s_t = x_{<t}$ \n",
    "+ **action** $a_t$ \n",
    "+ $\\pi_\\theta$ term is the **unconditional policy**\n",
    "    - Unconditional Policy: $\\pi_{\\theta}(a_t|s_t)=softmax(h^{\\theta}_{<t}$)\n",
    "\n",
    "+ $\\pi_{\\theta_c}$ is the **conditional policy**\n",
    "    + the only difference is that we are using the conditional parameter in the hidden states\n",
    "    - Conditional Policy: $\\pi_{\\theta_c}(a_t|s_t)=softmax(h^{\\theta_c}_{<t})$\n",
    "\n",
    "+ $G(x^c_t)$ **Salience Gain** of specified class label\n",
    "    + salience gain is determined by the specified class label\n",
    "    + the salience gain measures how closely the generated token resembles the salient lexicon of the target label and serves as a guide signal for the conditional generation \n",
    "    + salient lexicon\n",
    "        + calculate the salience score for each word of a vocabular and pick the top-N highest words as the salient lexicon $w_c$ for all class label. \n",
    "        + N is a hpyerparameter\n",
    "    \n",
    "\n",
    "+ **KL Divergence** ($KL(\\theta ||\\theta_c)$) tries to guarantee the conditional policy and uncontrolled policy are not too far away\n",
    "\n",
    "+ **Policy Gradient**:  two changes to classic SGD\n",
    "    + **temperatur T** to control the strength of the updates\n",
    "    + **accumulate K steps** during this reinforcement learning procedure\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/PolicyGradient.png\" alt=\"PolicyGradient\" style=\"zoom:65%;\" />\n",
    "<br><br>\n",
    "\n",
    "\n",
    "### Results/Findings: \n",
    "\n",
    "Evaluation on three different Tasks\n",
    "+ **Irony Classification, Sentiment Analysis, Offense Detection**\n",
    "+ datasets with 2, 3 and 4 classes\n",
    "+ different in size\n",
    "\n",
    "\n",
    "**Improvement**: \n",
    "+ **fine-tuned BERT** as the judge classifier\n",
    "+ Data Boost can even **improve** the performance of large-scale language model based classifier, like BERT.\n",
    "+ much improvement at extreme data scarcity cases - about **10 percent**\n",
    "+ achieved absolute **F1** increase\n",
    "  + **F1-score** is a measure of a test's accuracy. It is the **harmonic mean** of the **precision** and **recall**\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/Improvement.png\" alt=\"Improvement\" style=\"zoom:50%;\" />\n",
    "<br><br>\n",
    "\n",
    "**Coherence** (Stimmigkeit / Zusammenhang)\n",
    "+ generated Data correspondence to the original class\n",
    "+ visualizing with t-SNE that is a statistical method for high-dimensional data (sentence to vector)\n",
    "+ Boost manages to guide the generation towards the target labels\n",
    "+ for the most part, the distribution of generated sentences matches that of the original data\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/Coherence.png\" alt=\"Coherence\" style=\"zoom:50%;\" />\n",
    "<br><br>\n",
    "\n",
    " **Robustness**\n",
    " + tested on 5 classifiers with different architectures, including state-of-the-art language models\n",
    " + helpful and efficient with different types of classifiers\n",
    " + Data Boost generally improves the performance of all the classifiers, regardless of the classifier architecture\n",
    " + also beneficial for complex LM-based classifiers, which are already trained on a large corpus and generally used as strong baselines for text classification tasks\n",
    " \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/robustness.png\" alt=\"robustness\" style=\"zoom:40%;\" />\n",
    "<br><br>\n",
    "\n",
    "**Related Work** \n",
    "+ word replacement methods\n",
    "  + outperformed by Data Boost\n",
    "  + word replacement brings limited improvement \n",
    "  + flawed syntactic structure\n",
    "  + low readability\n",
    "+ other works mostly have higher perplexity\n",
    "  + due to partial generation\n",
    "  + does not take context into account\n",
    "  + (Perplexity is a measurement of how well a probability distribution or probability model predicts a sample. A low perplexity indicates the probability distribution is good at predicting the sample.)\n",
    "+ Data Boost outperforms all of them\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/related-work.png\" alt=\"related-work\" style=\"zoom:50%;\" />\n",
    "<br><br>\n",
    "\n",
    "**Limitations**\n",
    "+ to extract explicit lexical features for the metaphor, sarcastic and formal classes\n",
    "  + could be because syntactic features play a role in these classes  \n",
    "  + Data Boost generation is token-by-token\n",
    "\n",
    "**Conclusion / Averages of Data Boost**:\n",
    "+ Data Boost generates sentence-level augmentation\n",
    "  + high readability and class consistency\n",
    "  + greater variety in terms of vocabulary and sentence structure.\n",
    "+ Data Boost is easy to deploy\n",
    "  + no need for external datasets or training separate systems from scratch\n",
    "+ Data Boost is powerful\n",
    "  + brings 10% F1 improvement using 10 % true data\n",
    "\n",
    "\n",
    "\n",
    "### Mentions / Discussion: \n",
    "\n",
    "+ Sometimes it is confusing which training data they used. (Original data only, original and augmented data or augmented data only)\n",
    "+ Human Evaluation -  Each participant was paid 75 cents for their participation in this study\n",
    "+ Curiously, for the neutral (Sentiment), abusive (Offense) and hateful (Offense) labels, both Data Boost and vanilla GPT-2 generated samples were rated as more readable the original samples.\n",
    "* The code will be made available upon request \n",
    "+ They mention that there are methods that have not yet been tried for text augmentation and that they do not use reinforcement learning to have fine-grained control over the generation, which they found particularly helpful when dealing with multiple labels within the same task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4521601",
   "metadata": {},
   "source": [
    "### A) Paper Presentation III: Anne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6442b0e",
   "metadata": {},
   "source": [
    "# Title: Reinforcement-Learning Based Portfolio Management with Augmented Asset Movement Prediction States \n",
    "\n",
    "#### Link: https://ojs.aaai.org//index.php/AAAI/article/view/5462\n",
    "\n",
    "### Summary\n",
    "- trading is a decision-making process which includes the continuous derivation of valuable information from various data sources and sequential decision optimization\n",
    "\n",
    "- The paper introduces a state augmented reinforcement learning framework (SARL) for portfolio management (PM) to adress data heterogenity and envionment uncertainty challenges of automated PM\n",
    "\n",
    "- SARL augments the asset information by their price movement prediction as additional states, whereby the prediction can be based exclusively on financial data (e.g. asset prices) or derived from alternative sources (such as news articles)\n",
    "\n",
    "- two experiments on two real data sets validate the effectiveness of SARL in comparison to state of the art reinforcement learning (RL) frameworks for trading:\n",
    "\n",
    " (i) Bitcoin market \n",
    " (ii) High-tech stock \n",
    "\n",
    "- extensive simulations are carried out to demonstrate the importance of the state enhancement, which provides new insights and significantly improves performance compared to the standard RL-based PM method and other baselines\n",
    "\n",
    "### Problem Definition\n",
    "- PM is still largely based on linear models and the Markowitz framework\n",
    "\n",
    "   -> relies on accurate prediction of market prices and restricted assumptions such as past probability distribution of assets’    returns fully representing the future\n",
    "\n",
    "\n",
    "   #### Challenges/Problems of recent RL methods for trading are:\n",
    "\n",
    " (1) data heterogenity \n",
    "\n",
    "    - due to the fact that information data, such as news articles are very unstructured\n",
    "    \n",
    " (2) Environment uncertainty\n",
    "\n",
    "    - the non-stationary nature of financial markets induces uncertainty and often causes a distribution shift between training and testing data\n",
    "    \n",
    "\n",
    "### Idea: An hierarchical approach which binds supervised learning and RL into a unified framework such that it can be trained with standard RL methods\n",
    "The framework aims to leverage additional diverse information from alternative sources (other than classical structured financial data) into augmented states which then will be the used for reinforcement learning\n",
    "\n",
    "--> The SARL framework can easily incorporate different data sources through the use of an encoder for state augmentation\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. employ an end-to-end network to extract asset movement information frogm internal source (e.g. price up/down predicted label from historical prices) or external source (e.g. news embedding)\n",
    "\n",
    "2. integrate it with the prices of assets for state augmentation\n",
    "\n",
    "3. adopt a deterministic policy gradient algorithm based on the augmented state for learning the policy of PM\n",
    "\n",
    "\n",
    "![alt text](https://d3i71xaburhd42.cloudfront.net/61bee52afa721d13982289497f3408e54444f85b/3-Figure1-1.png \" \")\n",
    "\n",
    "#### Example proceeding for external features from news articles: \n",
    "\n",
    "- collect financial news articles related to the assets \n",
    "\n",
    "- use different kinds of Natural Language Processing methods as encoders to embed the news\n",
    "\n",
    "- feed the embedding into a hierarchical attention network (HAN) to train a binary classifier to predict the price movement\n",
    "\n",
    "- the features in the last layer before the softmax layer are extracted to represent the embedding of the news\n",
    "\n",
    "- integrate the embedding into the state for augmentation\n",
    "\n",
    "\n",
    "### State Augmentation Framework:\n",
    "#### The Augmented Asset Movement Prediction State\n",
    "\n",
    "\n",
    "s* = observable state (i.e. current asset price)\n",
    "\n",
    "encoder δ: summarizes asset movement prediction from past asset prices or news\n",
    "\n",
    "    - the encoder δ takes different types of data sources and transforms their contents into informatve representations to be augmented to the asset prices for training an RL agent\n",
    "\n",
    "    - example encoder: a feature extraction function derived from a text classifier, which is trained on the word embeddings of news for asset movement prediction\n",
    "augmented state s: s = (s*,δ)\n",
    "\n",
    "\n",
    "--> Goal: Train a recurrent neural network with long short-term memory to predict the asset movement. The binary output (price up/down) will guide the model to choose a better strategy.\n",
    "\n",
    "\n",
    "### RL Framework:\n",
    "** Environment: ** market consisting of all the assets for PM and other Information\n",
    "\n",
    "** Agent: ** Algorithm which observes the environment and makes decisions to interact with the market and rebalance the portfolio\n",
    "\n",
    "** Internal data source: ** asset prices\n",
    "\n",
    "** External data source: ** financial news articles\n",
    "\n",
    "** State Space: **\n",
    "- augmented asset prices\n",
    "\n",
    "** Action Space: **\n",
    "- what the agent should do is to reallocate the assets into assets -> adjust the vector of total assets \n",
    "\n",
    "- The desired reallocating weights at a time step defines the action vector\n",
    "\n",
    "** Reward Function: **\n",
    "- defined based on profit the agent made\n",
    "- accumulated product value (equivalent to maximize the sum of the logarithmic value)\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "**Figure 3:** The portfolio value of different PM methods. SARL is our proposed state augmentation RL method, DPM is the state-of-art standard RL method in PM. CRP, OL- MAR, WMAMR are baseline financial PM methods.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/main/materials/images/portfolio_values.png\" alt=\"Portfolio Values\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "--> SARL improves PV by 140.9% and 15.7% when compared to the state-of- art RL algorithm for PM (DPM)\n",
    "\n",
    "**Table 1: ** The training and testing accuracy of the text classifier for different word embedding methods.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/61bee52afa721d13982289497f3408e54444f85b/6-Table1-1.png\" alt=\"sparsity\" style=\"width: 480px;\"/>\n",
    "\n",
    "--> the best training/testing accuracy is no greater than 66%/61%, which leads to suggesting noisy information in the collected news\n",
    "\n",
    "** Figure 5:** The sparsity test on Bitcoin dataset. Each subfig- ure shows the results of SARL whose state is augmented by simulated price prediction labels with different sparsity. LD is label density from {20%, 50%, 80%, 100%}. \n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/61bee52afa721d13982289497f3408e54444f85b/500px/7-Figure5-1.png\" alt=\"sparsity\" style=\"width: 480px;\"/>\n",
    "\n",
    "\n",
    "--> The performance of PM benefits from higher label density\n",
    "--> The figure shows that even if there are only labels with density 50% but 70% accuracy, the portfolio value still has an improvement of 90.3% when compared to the state of the art (DPM). This suggests that if there are labels with high confidence, an improvement of the PM strategy is possible even if the labels are sparse\n",
    "\n",
    "** Figure 6:** The comparison between states augmented by dif- ferent news embedding and randomly simulated labels.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/61bee52afa721d13982289497f3408e54444f85b/7-Figure6-1.png\"  style=\"width: 480px;\"/>\n",
    "\n",
    "--> verifies the existence of the hidden correlation in financial news\n",
    "\n",
    "### Results\n",
    "- SARL outperforms the state-of- the-art method by 140.9% on the Bitcoin dataset and 15.7% on the HighTech dataset\n",
    "\n",
    "- SARL is more generalizable than the former regarding data distribution shift\n",
    "\n",
    "\n",
    "The experiments show that:\n",
    "\n",
    "(1) The exploitation of diverse information can reduce the impact of environment uncertainty\n",
    "\n",
    " (2) High-density (more frequent) external information can help boost the overall PM performance even when the information is noisy\n",
    "\n",
    "(3) Low-density but high-accuracy external information can also improve the PM performance\n",
    "\n",
    "\n",
    "### Critical Discussion\n",
    "- **++** The paper is easy to understand due to a clear structure and informative visualisation \n",
    "\n",
    "- **++** Sheds light onto a very interesting field wich can be expanded for example by discussion of language processing (i.e. detecting positive/negative tones in news articles) for the encoding\n",
    "\n",
    "- **-** The used encoders are not presented in the paper\n",
    "\n",
    "- **-** The authors don't explain how the stock movement signal takes impact on the augmented state and how high the predicted price movement is due to the encoders  \n",
    "\n",
    "- **-** There ist no primary explanation why it may be useful to first predict the state, as it turns out to be\n",
    "\n",
    "- **-** No information about how well the agent performed (i.e. in comparison to the real course rates) \n",
    "\n",
    "- **-** little extensive conclusion/interpretation and no advices for future research\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd8dbf",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9266049",
   "metadata": {},
   "source": [
    "## C) Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102a431",
   "metadata": {},
   "source": [
    "#### Examples III\n",
    "\n",
    "Game: Frozen Lake\n",
    "\n",
    "I am using the gym environment of openai today. \n",
    "You can install it this way: \n",
    "\n",
    "`conda install -c conda-forge gym`\n",
    "\n",
    "The game we try to solve is the frozen lake game:\n",
    "<img src=\"https://camo.githubusercontent.com/f558f268f3c1a45f0a88342113476f34ce894896c30b66fdc3101c8d090a0a0a/68747470733a2f2f616e616c7974696373696e6469616d61672e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30332f46726f7a656e2d4c616b652e706e67\" width=\"500\"/>\n",
    "\n",
    "For this we use the openai gym environment implementation from here: \n",
    "https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "You can find the source code in this repository:\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "\n",
    "The environment is an implementation of the same interface as we implemented last week. \n",
    "\n",
    "Let's start:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2569c7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check GPU reachability \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6878fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "import random \n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd18d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an instance of the frozen lake gym environment\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d70cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "#print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "#print(action_space_size)\n",
    "state_space = env.observation_space \n",
    "print(state_space)\n",
    "state_space_size = env.observation_space.n\n",
    "print(state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69e86aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-TABLE\n",
    "# build our action-value table | Q-TABLE\n",
    "# as you already know, the q-table looks like this\n",
    "# state | action_space\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "#print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3708e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "num_test_episodes = 3     \n",
    "\n",
    "# q-learning | update parameters\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# exploration-exploitation trade off\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f6822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-LEARNING\n",
    "\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = env.reset()\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > exploration_rate):\n",
    "            action = np.argmax(q_table[state, : ])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-Table Q(s,a) using the bellman update  \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, : ]))\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6318d82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****INFO: average reward per thousand episodes: ***** \n",
      "\n",
      "1000 :  0.05100000000000004\n",
      "2000 :  0.23100000000000018\n",
      "3000 :  0.3930000000000003\n",
      "4000 :  0.5500000000000004\n",
      "5000 :  0.6230000000000004\n",
      "6000 :  0.7040000000000005\n",
      "7000 :  0.6530000000000005\n",
      "8000 :  0.6980000000000005\n",
      "9000 :  0.6740000000000005\n",
      "10000 :  0.7010000000000005\n",
      "\n",
      "\n",
      " ***** Q-TABLE ***** \n",
      "\n",
      "[[0.6032918  0.52051926 0.52757518 0.52314479]\n",
      " [0.27564079 0.31121611 0.37842906 0.53454468]\n",
      " [0.40776611 0.42450012 0.41705814 0.47372005]\n",
      " [0.29044264 0.29152389 0.27231605 0.45458253]\n",
      " [0.62139108 0.40005343 0.42102558 0.36363938]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41566046 0.174966   0.19631856 0.08520832]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.46381587 0.36760085 0.51062419 0.66496301]\n",
      " [0.30029783 0.73460421 0.52349917 0.43820418]\n",
      " [0.65238002 0.34436335 0.45619523 0.33578276]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.25909959 0.44331484 0.84152439 0.40159336]\n",
      " [0.7628592  0.94548667 0.81317354 0.72951769]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print our learned q-table\n",
    "print(\"\\n\\n ***** Q-TABLE ***** \\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent died.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c68104",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "Game: Taxi \n",
    "\n",
    "Find the OpenAI Gym Environment here: https://gym.openai.com/envs/Taxi-v3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19918e",
   "metadata": {},
   "source": [
    "### ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29067d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN = the data_object (JSON SERIALIZABLE)\n",
    "class Environment():\n",
    "    \"\"\"The main Environment class. It encapsulates an environment with\n",
    "    arbitrary behind-the-scenes dynamics. An environment can be\n",
    "    partially or fully observed.\n",
    "    The main API methods that users of this class need to know are:\n",
    "        step\n",
    "        reset\n",
    "        render\n",
    "        close\n",
    "        seed\n",
    "    And set the following attributes:\n",
    "        action_space: The Space object corresponding to valid actions\n",
    "        observation_space: The Space object corresponding to valid observations\n",
    "        reward_range: A tuple corresponding to the min and max possible rewards\n",
    "    Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n",
    "    The methods are accessed publicly as \"step\", \"reset\", etc...\n",
    "    \"\"\"\n",
    "    # Set this in SOME subclasses\n",
    "    metadata = {'render.modes': []}\n",
    "    reward_range = (-float('inf'), float('inf'))\n",
    "    spec = None\n",
    "\n",
    "    def __init__(self, action_space=None, observation_space=None):\n",
    "        # Set variables\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "    \n",
    "# REPOSITORY = the functionality interface\n",
    "class EnvironmentRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        Args:\n",
    "            action (object): an action provided by the agent\n",
    "        Returns:\n",
    "            observation (object): agent's observation of the current environment\n",
    "            reward (float) : amount of reward returned after previous action\n",
    "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to an initial state and returns an initial\n",
    "        observation.\n",
    "        Note that this function should not reset the environment's random\n",
    "        number generator(s); random variables in the environment's state should\n",
    "        be sampled independently between multiple calls to `reset()`. In other\n",
    "        words, each call of `reset()` should yield an environment suitable for\n",
    "        a new episode, independent of previous episodes.\n",
    "        Returns:\n",
    "            observation (object): the initial observation.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.) By convention,\n",
    "        if mode is:\n",
    "        - human: render to the current display or terminal and\n",
    "          return nothing. Usually for human consumption.\n",
    "        - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
    "          representing RGB values for an x-by-y pixel image, suitable\n",
    "          for turning into a video.\n",
    "        - ansi: Return a string (str) or StringIO.StringIO containing a\n",
    "          terminal-style text representation. The text can include newlines\n",
    "          and ANSI escape sequences (e.g. for colors).\n",
    "        Note:\n",
    "            Make sure that your class's metadata 'render.modes' key includes\n",
    "              the list of supported modes. It's recommended to call super()\n",
    "              in implementations to use the functionality of this method.\n",
    "        Args:\n",
    "            mode (str): the mode to render with\n",
    "        Example:\n",
    "        class MyEnv(Env):\n",
    "            metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "            def render(self, mode='human'):\n",
    "                if mode == 'rgb_array':\n",
    "                    return np.array(...) # return RGB frame suitable for video\n",
    "                elif mode == 'human':\n",
    "                    ... # pop up a window and render\n",
    "                else:\n",
    "                    # just raise an exception\n",
    "                    super(MyEnv, self).render(mode=mode)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        \"\"\"Override close in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b447827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPOSITORY IMPLEMENTATION = the way how you would like to implement it\n",
    "class EnvironmentRepositoryImpl(EnvironmentRepository):\n",
    "    # Initialize / Instance Attributes\n",
    "    def __init__(self, environment):\n",
    "        # Set variables\n",
    "        self.data_object = environment\n",
    "        print('Environment initialized')\n",
    "\n",
    "    def step(self, action):\n",
    "        state = self.data_object.step(action)\n",
    "        return state\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.data_object.reset()\n",
    "        return state\n",
    "\n",
    "    def render(self):\n",
    "        self.data_object.render()\n",
    "\n",
    "    def close(self):\n",
    "        state = self.data_object.close()\n",
    "\n",
    "    def get_action_space(self):\n",
    "        # get action space from api of the playground or via js in browser using selenium\n",
    "        action_space = self.data_object.action_space\n",
    "        return action_space\n",
    "\n",
    "    def get_observation_space(self):\n",
    "        # get observation space of the playground from api or via js in browser using selenium\n",
    "        observation_space = self.data_object.observation_space\n",
    "        return observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4899f6c",
   "metadata": {},
   "source": [
    "### AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d767400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN\n",
    "class Agent():\n",
    "    # class variables\n",
    "    agent_variable = \"\"\n",
    "    # class methods\n",
    "    def __init__(self, agent_variable=\"\"):\n",
    "        self.agent_variable = agent_variable\n",
    "        \n",
    "# REPOSITORY\n",
    "class AgentRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def get_action(self, state): # This is the agent's POLICY, if you will\n",
    "        \"\"\" Agent gets a state as input and returns an action \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4f215b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPOSITORY IMPLEMENTATION\n",
    "class AgentRepositoryImpl(AgentRepository):\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, environment, agent):\n",
    "        # Set variables\n",
    "        self.data_object = agent\n",
    "        self.environment = environment\n",
    "        self.action_space = self.environment.action_space\n",
    "        self.observation_space = self.environment.observation_space\n",
    "        self.action_space_size = self.environment.action_space.n\n",
    "        self.observation_space_size = self.environment.observation_space.n\n",
    "        # INIT Q-TABLE\n",
    "        self.q_table = np.zeros((self.observation_space_size, self.action_space_size))\n",
    "        # INIT AGENT PARAMETERS\n",
    "        self.learning_rate = 0.7           # Learning rate\n",
    "        self.discount_rate = 0.618         # Discounting rate\n",
    "        self.exploration_rate = 1.0        # Exploration rate\n",
    "        self.max_exploration_rate = 1.0    # Exploration probability at start\n",
    "        self.min_exploration_rate = 0.01   # Minimum exploration probability \n",
    "        self.exploration_decay_rate = 0.01 # Exponential decay rate for exploration probability\n",
    "        print('Agent initialized.')\n",
    "\n",
    "    def get_action(self, state):\n",
    "        #EXPLORATION-EXPLOITATION TRADE OFF\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > self.exploration_rate):\n",
    "            # get action from q table\n",
    "            action = np.argmax(self.q_table[state, : ])\n",
    "        else:\n",
    "            # get random action\n",
    "            action = self.get_random_action()\n",
    "        return action\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        #action_set = random.sample(self.action_space, 1)\n",
    "        #action = action_set[0]\n",
    "        action = self.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, new_state):\n",
    "        self.q_table[state, action] = self.q_table[state, action] * (1 - self.learning_rate) + self.learning_rate * (reward + self.discount_rate * np.max(self.q_table[new_state, : ]))\n",
    "        \n",
    "    def update_exploration_rate(self, episode_num):\n",
    "        self.exploration_rate = self.min_exploration_rate + (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate*episode_num)\n",
    "    \n",
    "    def get_exploit_action(self, state):\n",
    "        action = np.argmax(self.q_table[state, : ])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08445249",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "99ce7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 50000        # Total episodes\n",
    "max_steps_per_episode = 100 # Max steps per episode\n",
    "num_test_episodes = 5     # Total test episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5aa78dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "6\n",
      "Discrete(500)\n",
      "500\n",
      "(-inf, inf)\n",
      "Environment initialized\n",
      "Agent initialized.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the Environment\n",
    "\n",
    "# get the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "print(action_space_size)\n",
    "observation_space = env.observation_space \n",
    "print(observation_space)\n",
    "observation_space_size = env.observation_space.n\n",
    "print(observation_space_size)\n",
    "reward_range = env.reward_range\n",
    "print(reward_range)\n",
    "\n",
    "environment_data_object = env #Environment(action_space, observation_space)\n",
    "environment = EnvironmentRepositoryImpl(environment_data_object)\n",
    "\n",
    "# Setting up the Agent\n",
    "agent_data_object = Agent()\n",
    "agent = AgentRepositoryImpl(environment_data_object, agent_data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2eeb93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = environment.reset()\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        action = agent.get_action(state)\n",
    "            \n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "        \n",
    "        # Update Q-Table Q(s,a) using the bellman update  \n",
    "        agent.update_q_table(state, action, reward, new_state)\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    agent.update_exploration_rate(episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ad15cbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****INFO: average reward per thousand episodes: ***** \n",
      "\n",
      "1000 :  -45.3959999999998\n",
      "2000 :  7.407999999999966\n",
      "3000 :  7.5049999999999635\n",
      "4000 :  7.351999999999967\n",
      "5000 :  7.470999999999966\n",
      "6000 :  7.561999999999962\n",
      "7000 :  7.5269999999999735\n",
      "8000 :  7.457999999999962\n",
      "9000 :  7.48799999999996\n",
      "10000 :  7.464999999999965\n",
      "11000 :  7.399999999999965\n",
      "12000 :  7.312999999999967\n",
      "13000 :  7.3119999999999585\n",
      "14000 :  7.335999999999961\n",
      "15000 :  7.421999999999961\n",
      "16000 :  7.3349999999999556\n",
      "17000 :  7.301999999999971\n",
      "18000 :  7.198999999999964\n",
      "19000 :  7.298999999999972\n",
      "20000 :  7.408999999999964\n",
      "21000 :  7.5029999999999575\n",
      "22000 :  7.471999999999971\n",
      "23000 :  7.283999999999966\n",
      "24000 :  7.396999999999963\n",
      "25000 :  7.408999999999962\n",
      "26000 :  7.49899999999996\n",
      "27000 :  7.361999999999972\n",
      "28000 :  7.333999999999961\n",
      "29000 :  7.434999999999964\n",
      "30000 :  7.4679999999999644\n",
      "31000 :  7.595999999999968\n",
      "32000 :  7.188999999999968\n",
      "33000 :  7.5039999999999605\n",
      "34000 :  7.349999999999961\n",
      "35000 :  7.42599999999996\n",
      "36000 :  7.511999999999971\n",
      "37000 :  7.358999999999963\n",
      "38000 :  7.316999999999972\n",
      "39000 :  7.415999999999971\n",
      "40000 :  7.351999999999968\n",
      "41000 :  7.441999999999956\n",
      "42000 :  7.342999999999967\n",
      "43000 :  7.316999999999961\n",
      "44000 :  7.54499999999997\n",
      "45000 :  7.552999999999964\n",
      "46000 :  7.4139999999999695\n",
      "47000 :  7.504999999999956\n",
      "48000 :  7.368999999999971\n",
      "49000 :  7.3769999999999705\n",
      "50000 :  7.327999999999974\n",
      "\n",
      "\n",
      " ***** Q-TABLE ***** \n",
      "\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -2.5042147   -2.43401533  -2.50423222  -2.43407224  -2.32039715\n",
      "  -11.43396515]\n",
      " [ -1.84008203  -1.35777551  -1.83919178  -1.35844496  -0.57891593\n",
      "  -10.35892069]\n",
      " ...\n",
      " [ -1.94631341   0.6813468   -1.95448955  -2.0084676   -7.\n",
      "   -7.        ]\n",
      " [ -2.35377987  -2.34048369  -2.29903226  -2.13656806  -9.1\n",
      "   -9.98592646]\n",
      " [ -0.7         -0.91        -0.7         11.36        -2.71168563\n",
      "    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print our learned q-table\n",
    "print(\"\\n\\n ***** Q-TABLE ***** \\n\")\n",
    "print(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2192348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        environment.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = agent.get_exploit_action(state)\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            environment.render()\n",
    "            if reward == 20:\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent missed the goal.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53af7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LITTLE HOMEWORK\n",
    "# 1. get an environment of the openai gym (e.g. cart pole, lunar lander, breakout)\n",
    "# 2. print the essential information about the environment (state space, action space, ...)\n",
    "# 3. write an agent class\n",
    "# 4. train your agent on the environment using Q-Learning (play around with the hyperparameters for your environment)\n",
    "# 5. Plot your results (average reward, q-table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c9044",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10738f0d",
   "metadata": {},
   "source": [
    "# TODO's\n",
    "\n",
    "1. Send your finished presentations (+ possibly annotated paper) by **Monday 12.00 AM/midnight** via email to henrik.voigt@uni-jena.de\n",
    "\n",
    "2. Send your little HOMEWORK to henrik.voigt@uni-jena.de by using the naming convention: HOMEWORK_02_FIRSTNAME_LASTNAME.ipynb until **June 9th 12.00 AM/midnight**\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env-kernel",
   "language": "python",
   "name": "pytorch-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
