{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff78b3c",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 10 Reinforcement Learning in der Sprachtechnologie\n",
    "## Supervised-Learning Based Speech Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe377a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Announcements\n",
    "\n",
    "#### Papers\n",
    "\n",
    "* New video out from Yannic Kilcher about Decision Transformers and their application in Reinforcement Learning: https://www.youtube.com/watch?v=-buULmf7dec\n",
    "\n",
    "#### Homework\n",
    "        \n",
    "* DEADLINE: JUNE 23th\n",
    "\n",
    "#### Today\n",
    "\n",
    "* Finishing the Deep-Q-Learning Example\n",
    "\n",
    "* Starting the Supvervised-Learning Based Speech Agent Example\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ee835",
   "metadata": {},
   "source": [
    "### A) Paper Presentation I: Bianca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e97f9",
   "metadata": {},
   "source": [
    "***\n",
    "# Grounding Natural Language Commands to StarCraft II Game States \n",
    "## for Narration-Guided Reinforcement Learning\n",
    "\n",
    "Source:\n",
    "\n",
    "Waytowich, Nicholas, et al. \"Grounding natural language commands to StarCraft II game states for narration-guided reinforcement learning.\" Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications. Vol. 11006. International Society for Optics and Photonics, 2019.\n",
    "\n",
    "(https://arxiv.org/pdf/1906.02671.pdf)\n",
    "\n",
    "## Aim of the paper\n",
    "* address the problem of reward sparsity\n",
    "* apply reward shaping using natural language (NL) narration\n",
    "* ground NL commands to goal-specific states by learning a mutual embedding space\n",
    "\n",
    "## The Game\n",
    "StarCraft II is a complex real-time strategy game \n",
    "\n",
    "https://www.youtube.com/watch?v=yaqeZ9Snt4E\n",
    "\n",
    "#### Why is StarCraft II challenging for RL?\n",
    "* large action spaces\n",
    "* environment state is only partially observable \n",
    "* long time horizons\n",
    "* sparse game score\n",
    "\n",
    "here: focus on the StarCraft II BuildMarines mini-game\n",
    "\n",
    "goal: build as many marines as possible\n",
    "\n",
    "to do that: sequences of actions are necessary: \n",
    "* build workers\n",
    "* collect resources\n",
    "* build supply depots\n",
    "* build barracks\n",
    "* train marines\n",
    "\n",
    "## Reward Sparsity\n",
    "Sparse reward functions have many zero reward values\n",
    "\n",
    "$+$ they are easier to specify: \n",
    " * 1 for winning the game, 0 for all other positions\n",
    " * no expert knowledge is necessary\n",
    "\n",
    "\n",
    "$-$ they lead to meagre results: \n",
    " * the agent receives little/no meaningful feedback\n",
    " * zero reward leads to no changes in the agent's policy\n",
    " * the agent takes many random actions and maybe stumbles into a meaningful goal state \n",
    " \n",
    "### State Space\n",
    "\n",
    "* 7 mini-map feature layers (size 64x64)\n",
    "* 13 screen feature layer maps (size 64x64) for a total of 20 2d images (size 64x64)\n",
    "* 13 non-spatial features with information such as player resources and build queues\n",
    "\n",
    "\n",
    "### Action Space\n",
    "\n",
    "* compound action consisting of \n",
    " * action identifier (action to be run)\n",
    " * two spatial actions (x and y), represented as two vectors (length 64)\n",
    " \n",
    "## The Mutual Embedding Model (MEM)\n",
    "\n",
    "* learns a mapping between game states and NL commands\n",
    "* this way, the agent can assign contextual meaning to its current game state (compare current state with desired goal state)\n",
    "\n",
    "\n",
    "### State embedding \n",
    "\n",
    "* extraction of visual features from the mini-maps and screens (2 layer CNNs)\n",
    "* non-spatial features passed through a fully connected layer\n",
    "* concatenation of mini-map, screen and non-spatial features \n",
    "* projection into an embedding space of size 256\n",
    "\n",
    "### Language embedding\n",
    "\n",
    "* using pretrained word2vec vectors (vocabulary size: 50,000, embedding size: 128)\n",
    "* for every word in a command, the word2vec embedding was extracted \n",
    "* then, a command-level embedding (size: 256) was trained in an LSTM \n",
    "\n",
    "\n",
    "### Mutual embedding\n",
    "\n",
    "* goal: game states and their corresponding language commands are close in the mutual embedding space \n",
    "* minimizing the euclidean distance of matching state and language vectors \n",
    "* maximizing the euclidean distance of mismatching state and language vectors \n",
    "\n",
    "\n",
    "### Dataset generation (for learning the embeddings): \n",
    "* a random agent generated pairs of game states and their matching commands using hand-crafted rules (the rules identify the states that satisfy the commands)\n",
    "* e.g. the command \"build a supply depot\" is satisfied when a new supply depot is created, this state is then labelled correspondingly \n",
    "\n",
    "* dimensions of the dataset:\n",
    " * 50,000 matched pairs (10,000 game states each for the 5 commands) \n",
    " * 50,000 mismatched pairs\n",
    " * 50,000 null states (not corresponding to any command) --> to distinguish desirable states from other states\n",
    " * --> 150,000 samples\n",
    " \n",
    "#### Splitting of the dataset:\n",
    "* training: 100,000 samples\n",
    "* validation: 25,000 samples\n",
    "* testing: 25,000 samples\n",
    "\n",
    "#### Training:\n",
    "* Optimizer: Adam\n",
    "* Learning rate: 0.0005\n",
    "* Batch size: 32\n",
    "* Epochs: 20\n",
    "* Threshold for the maximum euclidean distance for command and state to be associated: 0.5\n",
    "\n",
    "#### Results:\n",
    "* training accuracy: 95.61% \n",
    "* validation accuracy: 82.35% \n",
    "* test accuracy: 80.40%\n",
    "\n",
    "### Visualization\n",
    "\n",
    "t-SNE Clustering (t-distributed stochastic neighbor embedding):\n",
    "\n",
    "* Point clouds = state examples\n",
    "* circular symbols = projection of the NL command embedding \n",
    "\n",
    "--> MEM learned how to distinguish between the possible goal states and how to recognize if its current state\n",
    "matches the desired state provided by the NL command\n",
    "\n",
    "### Discussion\n",
    "\n",
    "* the MEM can ground natural language goals in an agentâ€™s state space \n",
    "* NL commands can be used to indicate desired goal states \n",
    "* thanks to shared representation, agent can compare current state with desired goal state\n",
    "* using human guidance via NL for a learning agent means a great potential:\n",
    "    * is flexible (\"build\" and \"construct\" should lead to same goal)\n",
    "    * sequential policies can be learned without the need of expert knowledge about reward functions \n",
    "\n",
    "$+$ clearly structured paper, well written, easy to understand\n",
    "\n",
    "$+$ using human guidance in an MEM seems to have great potential to avoid reward sparsity and enable better and faster learning\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531f75f",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c9040",
   "metadata": {},
   "source": [
    "## C) Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac44a8",
   "metadata": {},
   "source": [
    " **RL in Python**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925f34b",
   "metadata": {},
   "source": [
    "### CartPole Environment\n",
    "\n",
    "Find the environment here: https://gym.openai.com/envs/CartPole-v0/\n",
    "\n",
    "For solving the CartPole environment, Q-Learning with two different approaches can be applied. \n",
    "\n",
    "The first approach would be to **discretize** the state space of the environment and based on this build a Q-Table following the Q-Learning algorithm. \n",
    "\n",
    "The second approach would be to **approximate** the Q-table with a neural network, due to the fact that a continous state space would lead to an infinite q-table. \n",
    "\n",
    "In the following there are 2 examples shown. \n",
    "\n",
    "Example 1 is an implementation of the Q-Learning algorithm using a discretization of the state space of the cartpole environment. \n",
    "\n",
    "Example 2 is an implementation of the Q-Learning algorithm that approximates the (infinite) q-table with a neural network. \n",
    "\n",
    "**MY TIPP FOR YOU:** If you are new to reinforcement learning and object oriented programming try to discretize your environments and use a simple, straigh forward implementation of the Q-Learning environment. If you are an advanced programmer and familiar to concepts of RL and ML when dealing with a continous state space of your environment use a neural network for approximating the q-table and build it into an agent which fits the openai interfaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18367ab9",
   "metadata": {},
   "source": [
    "#### Discretization Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec996cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "import random \n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5febbf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an instance of the frozen lake gym environment\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc19d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "2\n",
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-676f4b2ed3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstate_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstate_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "print(action_space_size)\n",
    "state_space = env.observation_space \n",
    "print(state_space)\n",
    "state_space_size = env.observation_space.n\n",
    "print(state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e023d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        CartPoleEnv\n",
       "\u001b[0;31mString form:\u001b[0m <CartPoleEnv<CartPole-v0>>\n",
       "\u001b[0;31mFile:\u001b[0m        /mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Description:\n",
       "    A pole is attached by an un-actuated joint to a cart, which moves along\n",
       "    a frictionless track. The pendulum starts upright, and the goal is to\n",
       "    prevent it from falling over by increasing and reducing the cart's\n",
       "    velocity.\n",
       "\n",
       "Source:\n",
       "    This environment corresponds to the version of the cart-pole problem\n",
       "    described by Barto, Sutton, and Anderson\n",
       "\n",
       "Observation:\n",
       "    Type: Box(4)\n",
       "    Num     Observation               Min                     Max\n",
       "    0       Cart Position             -4.8                    4.8\n",
       "    1       Cart Velocity             -Inf                    Inf\n",
       "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
       "    3       Pole Angular Velocity     -Inf                    Inf\n",
       "\n",
       "Actions:\n",
       "    Type: Discrete(2)\n",
       "    Num   Action\n",
       "    0     Push cart to the left\n",
       "    1     Push cart to the right\n",
       "\n",
       "    Note: The amount the velocity that is reduced or increased is not\n",
       "    fixed; it depends on the angle the pole is pointing. This is because\n",
       "    the center of gravity of the pole increases the amount of energy needed\n",
       "    to move the cart underneath it\n",
       "\n",
       "Reward:\n",
       "    Reward is 1 for every step taken, including the termination step\n",
       "\n",
       "Starting State:\n",
       "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
       "\n",
       "Episode Termination:\n",
       "    Pole Angle is more than 12 degrees.\n",
       "    Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
       "    the display).\n",
       "    Episode length is greater than 200.\n",
       "    Solved Requirements:\n",
       "    Considered solved when the average return is greater than or equal to\n",
       "    195.0 over 100 consecutive trials.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892e0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-3.42857143, -2.05714286, -0.68571429,  0.68571429,  2.05714286,\n",
      "        3.42857143]), array([-2.14285714, -1.28571429, -0.42857143,  0.42857143,  1.28571429,\n",
      "        2.14285714]), array([-0.35714286, -0.21428571, -0.07142857,  0.07142857,  0.21428571,\n",
      "        0.35714286]), array([-1.42857143, -0.85714286, -0.28571429,  0.28571429,  0.85714286,\n",
      "        1.42857143])]\n",
      "6\n",
      "2401\n",
      "[ 0.01706873  0.00587641  0.04778642 -0.0470228 ]\n",
      "0.017068731149897334\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "# State Discretizer\n",
    "def discretize_range(lower_bound, upper_bound, num_bins):\n",
    "    return np.linspace(lower_bound, upper_bound, num_bins + 1)[1:-1]\n",
    "\n",
    "# Discretize the continuous state space for each of the 4 features.\n",
    "num_discretization_bins = 7\n",
    "state_bins = [\n",
    "    # Cart position.\n",
    "    discretize_range(-4.8, 4.8, num_discretization_bins),\n",
    "    # Cart velocity.\n",
    "    discretize_range(-3.0, 3.0, num_discretization_bins),\n",
    "    # Pole angle.\n",
    "    discretize_range(-0.5, 0.5, num_discretization_bins),\n",
    "    # Tip velocity.\n",
    "    discretize_range(-2.0, 2.0, num_discretization_bins)\n",
    "]\n",
    "print(state_bins)\n",
    "max_bins = max(len(bin) for bin in state_bins)\n",
    "print(max_bins)\n",
    "num_states = (max_bins + 1) ** len(state_bins)\n",
    "print(num_states)\n",
    "# our state space therefore has a size of 7x7x7x7 ~2500\n",
    "state_space_size = num_states\n",
    "\n",
    "# lets see how that looks like in practice\n",
    "state = env.reset()\n",
    "print(state)\n",
    "print(state[0])\n",
    "\n",
    "# discretize a given state into our state model, found here: https://www.statology.org/numpy-digitize/\n",
    "def discretize_value(value, bins):\n",
    "    return np.digitize(x=value, bins=bins)\n",
    "\n",
    "def discretize_state(observation):\n",
    "        # Discretize the observation features and reduce them to a single integer\n",
    "        #for i, feature in enumerate(observation):\n",
    "            #getting the bins\n",
    "            #print(state_bins[i]) \n",
    "            # extending to the max number of bins\n",
    "            #print(state_bins[i] * ((max_bins + 1)))\n",
    "            # putting it into the power of the decimal place \n",
    "            #print(state_bins[i] * ((max_bins + 1)) ** i)\n",
    "            # this encoding enables us to reach that: first states encodes one potency, second state encodes ten potency, third state encodes hundred potency, ...\n",
    "            #print(discretize_value(feature, state_bins[i]) * ((max_bins + 1) ** i))\n",
    "            \n",
    "        state = sum(\n",
    "            discretize_value(feature, state_bins[i]) * ((max_bins + 1) ** i)\n",
    "            for i, feature in enumerate(observation)\n",
    "        )\n",
    "        return state\n",
    "    \n",
    "test = discretize_state(state)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df0ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-TABLE\n",
    "# build our action-value table | Q-TABLE\n",
    "# as you already know, the q-table looks like this\n",
    "# state | action_space\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "#print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b3b34ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "num_test_episodes = 1     \n",
    "\n",
    "# q-learning | update parameters\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# exploration-exploitation trade off\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9c2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-LEARNING\n",
    "\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = env.reset()\n",
    "    # discretize state\n",
    "    state = discretize_state(state)\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > exploration_rate):\n",
    "            action = np.argmax(q_table[state, : ])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # discretize the observed state\n",
    "        new_state = discretize_state(new_state)\n",
    "        \n",
    "        # Update Q-Table Q(s,a) using the bellman update  \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, : ]))\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e98e4390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****INFO: average reward per thousand episodes: ***** \n",
      "\n",
      "1000 :  37.593000000000025\n",
      "2000 :  68.66300000000015\n",
      "3000 :  72.87700000000011\n",
      "4000 :  89.27999999999942\n",
      "5000 :  94.194999999999\n",
      "6000 :  95.04699999999899\n",
      "7000 :  94.62699999999899\n",
      "8000 :  95.81299999999922\n",
      "9000 :  98.4559999999988\n",
      "10000 :  92.45599999999955\n",
      "\n",
      "\n",
      " ***** Q-TABLE ***** \n",
      "\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print our learned q-table\n",
    "print(\"\\n\\n ***** Q-TABLE ***** \\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5cb8ee2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-69cef5badf34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1898\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1899\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    # discretize state\n",
    "    state = discretize_state(state)\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # discretize new state\n",
    "        new_state = discretize_state(new_state)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1: # check reward from environment for correct display\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent did not reach the goal.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04356950",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ea5f3",
   "metadata": {},
   "source": [
    "#### Deep Q Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bcbc9a",
   "metadata": {},
   "source": [
    "![alt_text](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fe66bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a349e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an instance of the frozen lake gym environment\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77998c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "2\n",
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "print(action_space_size)\n",
    "state_space = env.observation_space \n",
    "print(state_space)\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "print(state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ba45ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "num_test_episodes = 1     \n",
    "\n",
    "# q-learning | update parameters\n",
    "discount_rate = 0.99\n",
    "\n",
    "# exploration-exploitation trade off\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "# q network parameters\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47c61ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork():\n",
    "    ''' Deep Q Neural Network Class. '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):\n",
    "            self.criterion = torch.nn.MSELoss()\n",
    "            self.model = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(state_dim, hidden_dim),\n",
    "                            torch.nn.LeakyReLU(),\n",
    "                            torch.nn.Linear(hidden_dim, hidden_dim*2),\n",
    "                            torch.nn.LeakyReLU(),\n",
    "                            torch.nn.Linear(hidden_dim*2, action_dim)\n",
    "                    )\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "    def update(self, state, y):\n",
    "            \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "            y_pred = self.model(torch.Tensor(state))\n",
    "            loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def predict(self, state):\n",
    "            \"\"\" Compute Q values for all actions using the DeepQNetwork \"\"\"\n",
    "            with torch.no_grad():\n",
    "                return self.model(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcb73115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our deep q network\n",
    "deep_q_network = DeepQNetwork(state_space_size, action_space_size, hidden_dim, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28502018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-LEARNING\n",
    "\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = env.reset()\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > exploration_rate):\n",
    "            q_values = deep_q_network.predict(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update our Q Network due to the reward we got\n",
    "        q_values = deep_q_network.predict(state).tolist() \n",
    "        # Update network weights using the last step only\n",
    "        q_values_next = deep_q_network.predict(new_state)\n",
    "        q_values[action] = reward + discount_rate * torch.max(q_values_next).item() # BELLMANN EQUATION -> APPROXIMATE THE GOLD ACTION -> find the best possible action in every state -> APPROXIMATION\n",
    "        deep_q_network.update(state, q_values)\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            q_values[action] = reward\n",
    "            # Update network weights\n",
    "            deep_q_network.update(state, q_values)\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "372b1e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****INFO: average reward per thousand episodes: ***** \n",
      "\n",
      "100 :  23.05\n",
      "200 :  28.470000000000006\n",
      "300 :  32.730000000000004\n",
      "400 :  41.63\n",
      "500 :  53.45999999999999\n",
      "600 :  67.58\n",
      "700 :  65.26999999999998\n",
      "800 :  74.92999999999999\n",
      "900 :  74.44\n",
      "1000 :  82.31000000000002\n"
     ]
    }
   ],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_hundred_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/100)\n",
    "count = 100\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_hundred_episodes:\n",
    "    print(count, \": \", str(sum(reward/100)))\n",
    "    count += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a13fa41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-e1482c61b8c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1898\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1899\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        q_values = deep_q_network.predict(state)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1: # check reward from environment for correct display\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent did not reach the goal.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78867967",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037d044",
   "metadata": {},
   "source": [
    "## Supervised Learning Based Speech Agents\n",
    "\n",
    "A good way to start with supervised learning based speech agents are seq2seq chatbots. \n",
    "\n",
    "The general idea behind these bots is the following: \n",
    "\n",
    "![alt_text](https://miro.medium.com/max/860/1*vGhoOtfPSuv3gEEWvTP47g.png)\n",
    "\n",
    "![alt_text](https://ftp.slidegeeks.com/pics/dgm/l/3/3d_chain_sequence_diagram_illustrating_4_steps_make_flowchart_powerpoint_templates_1.jpg)\n",
    "\n",
    "***\n",
    "\n",
    "![alt_text](https://miro.medium.com/max/1928/1*CkeGXClZ5Xs0MhBc7xFqSA.png)\n",
    "\n",
    "You can find an implementation using RNNs here: https://github.com/praeclarumjj3/Chatbot-with-Pytorch\n",
    "\n",
    "***\n",
    "\n",
    "![alt_text](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
    "\n",
    "***\n",
    "\n",
    "My implementation is based on a transformer model. \n",
    "\n",
    "Nevertheless, you can also plug-in an RNN based approach and play around with that. \n",
    "\n",
    "Two repositories I considered and I can recommend are: https://github.com/fawazsammani/chatbot-transformer or https://github.com/jfriedson/Seq2Seq-Chatbot\n",
    "\n",
    "***\n",
    "\n",
    "There is detailed tutorial on that from pytorch themsleves here: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html or https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/chatbot_tutorial.ipynb\n",
    "\n",
    "For using pre-trained transformer models you can find a some interesting ideas on that here: https://www.thepythoncode.com/article/conversational-ai-chatbot-with-huggingface-transformers-in-python\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "117d32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "# We use the Cornell Movie Dialogue Corpus, which is a question-answer dataset having movies as the topic. \n",
    "# A description of the corpus can be found here: https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "# You can find it here: http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
    "\n",
    "# Pre-processing\n",
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "corpus_movie_conv = 'data/movie_conversations.txt'\n",
    "corpus_movie_lines = 'data/movie_lines.txt'\n",
    "max_len = 25\n",
    "\n",
    "# load conversations\n",
    "with open(corpus_movie_conv, 'r') as c:\n",
    "    conv = c.readlines()\n",
    "\n",
    "# load movie lines\n",
    "with open(corpus_movie_lines, 'r') as l:\n",
    "    lines = l.readlines()\n",
    "    \n",
    "# split and filter\n",
    "lines_dic = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dic[objects[0]] = objects[-1]\n",
    "    \n",
    "def remove_punc(string):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char  # space is also a character\n",
    "    return no_punct.lower()\n",
    "\n",
    "pairs = []\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        if i==len(ids)-1:\n",
    "            break\n",
    "        first = remove_punc(lines_dic[ids[i]].strip())      \n",
    "        second = remove_punc(lines_dic[ids[i+1]].strip())\n",
    "        qa_pairs.append(first.split()[:max_len])\n",
    "        qa_pairs.append(second.split()[:max_len])\n",
    "        pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7d6380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are: 18238\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "word_freq = Counter()\n",
    "for pair in pairs:\n",
    "    word_freq.update(pair[0])\n",
    "    word_freq.update(pair[1])\n",
    "    \n",
    "min_word_freq = 5\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "word_map['<pad>'] = 0\n",
    "\n",
    "print(\"Total words are: {}\".format(len(word_map)))\n",
    "\n",
    "# write vocabulary to file\n",
    "with open('data/WORDMAP_corpus.json', 'w') as j:\n",
    "    json.dump(word_map, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "298681fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dialogue pairs for training\n",
    "def encode_question(words, word_map):\n",
    "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c\n",
    "\n",
    "def encode_reply(words, word_map):\n",
    "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n",
    "    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c\n",
    "\n",
    "pairs_encoded = []\n",
    "for pair in pairs:\n",
    "    qus = encode_question(pair[0], word_map)\n",
    "    ans = encode_reply(pair[1], word_map)\n",
    "    pairs_encoded.append([qus, ans])\n",
    "    \n",
    "# write pairs to file\n",
    "with open('data/pairs_encoded.json', 'w') as p:\n",
    "    json.dump(pairs_encoded, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4e69df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch data set class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.pairs = json.load(open('data/pairs_encoded.json'))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        question = torch.LongTensor(self.pairs[i][0])\n",
    "        reply = torch.LongTensor(self.pairs[i][1])\n",
    "        return question, reply\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bea6276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch data loader for training\n",
    "train_loader = torch.utils.data.DataLoader(Dataset(),\n",
    "                                           batch_size = 100, \n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c65461d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "# we are using a standard transformer model in this tutorial, BUT you could apply ANY sequence to sequence model here\n",
    "def create_masks(question, reply_input, reply_target):\n",
    "    def subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    question_mask = question!=0\n",
    "    question_mask = question_mask.to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
    "    reply_input_mask = reply_input!=0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
    "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
    "    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n",
    "    return question_mask, reply_input_mask, reply_target_mask\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements embeddings of the words and adds their positional encodings. \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len = 50):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):   # for each position of the word\n",
    "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)   # include the batch size\n",
    "        return pe\n",
    "        \n",
    "    def forward(self, encoded_words):\n",
    "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
    "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, 512)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, 512)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
    "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
    "        weights = self.dropout(weights)\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        # (batch_size, max_len, h * d_k)\n",
    "        interacted = self.concat(context)\n",
    "        return interacted\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, heads, num_layers, word_map):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embed = Embeddings(self.vocab_size, d_model)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_words, src_mask):\n",
    "        src_embeddings = self.embed(src_words)\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
    "        tgt_embeddings = self.embed(target_words)\n",
    "        for layer in self.decoder:\n",
    "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "        \n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a78a8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# transformer specific warmup for quicker weight adaptation in the first epochs\n",
    "class AdamWarmup:\n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "    def step(self):\n",
    "        # Increment the number of steps each time we call the step function\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # update the learning rate\n",
    "        self.lr = lr\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Loss function\n",
    "class LossWithLS(nn.Module):\n",
    "    def __init__(self, size, smooth):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        self.confidence = 1.0 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, prediction, target, mask):\n",
    "        \"\"\"\n",
    "        prediction of shape: (batch_size, max_words, vocab_size)\n",
    "        target and mask of shape: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
    "        mask = mask.float()\n",
    "        mask = mask.view(-1)       # (batch_size * max_words)\n",
    "        labels = prediction.data.clone()\n",
    "        labels.fill_(self.smooth / (self.size - 1))\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "        \n",
    "# hyperparameters\n",
    "d_model = 512\n",
    "heads = 8\n",
    "num_layers = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 1\n",
    "\n",
    "with open('data/WORDMAP_corpus.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "    \n",
    "transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map)\n",
    "transformer = transformer.to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "criterion = LossWithLS(len(word_map), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09f1fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, transformer, criterion, epoch):    \n",
    "    transformer.train()\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, (question, reply) in enumerate(train_loader):\n",
    "        samples = question.shape[0]\n",
    "        # Move to device\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "        # Prepare Target Data\n",
    "        reply_input = reply[:, :-1]\n",
    "        reply_target = reply[:, 1:]\n",
    "        # Create mask and add dimensions\n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
    "        # Get the transformer outputs\n",
    "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
    "        # Compute the loss\n",
    "        loss = criterion(out, reply_target, reply_target_mask)\n",
    "        # Backprop\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "        sum_loss += loss.item() * samples\n",
    "        count += samples\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n",
    "            \n",
    "\n",
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    \"\"\"\n",
    "    Performs Greedy Decoding with a batch size of 1\n",
    "    \"\"\"\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "    transformer.eval()\n",
    "    start_token = word_map['<start>']\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device)\n",
    "    \n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[1]\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        _, next_word = torch.max(predictions, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        if next_word == word_map['<end>']:\n",
    "            break\n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
    "        \n",
    "    # Construct Sentence\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n",
    "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6863619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/2217]\tLoss: 8.694\n",
      "Epoch [0][100/2217]\tLoss: 7.915\n",
      "Epoch [0][200/2217]\tLoss: 7.217\n",
      "Epoch [0][300/2217]\tLoss: 6.678\n",
      "Epoch [0][400/2217]\tLoss: 6.315\n",
      "Epoch [0][500/2217]\tLoss: 6.061\n",
      "Epoch [0][600/2217]\tLoss: 5.876\n",
      "Epoch [0][700/2217]\tLoss: 5.728\n",
      "Epoch [0][800/2217]\tLoss: 5.609\n",
      "Epoch [0][900/2217]\tLoss: 5.511\n",
      "Epoch [0][1000/2217]\tLoss: 5.429\n",
      "Epoch [0][1100/2217]\tLoss: 5.357\n",
      "Epoch [0][1200/2217]\tLoss: 5.294\n",
      "Epoch [0][1300/2217]\tLoss: 5.240\n",
      "Epoch [0][1400/2217]\tLoss: 5.193\n",
      "Epoch [0][1500/2217]\tLoss: 5.152\n",
      "Epoch [0][1600/2217]\tLoss: 5.113\n",
      "Epoch [0][1700/2217]\tLoss: 5.079\n",
      "Epoch [0][1800/2217]\tLoss: 5.048\n",
      "Epoch [0][1900/2217]\tLoss: 5.020\n",
      "Epoch [0][2000/2217]\tLoss: 4.994\n",
      "Epoch [0][2100/2217]\tLoss: 4.970\n",
      "Epoch [0][2200/2217]\tLoss: 4.947\n",
      "Epoch [1][0/2217]\tLoss: 4.387\n",
      "Epoch [1][100/2217]\tLoss: 4.419\n",
      "Epoch [1][200/2217]\tLoss: 4.417\n",
      "Epoch [1][300/2217]\tLoss: 4.413\n",
      "Epoch [1][400/2217]\tLoss: 4.415\n",
      "Epoch [1][500/2217]\tLoss: 4.415\n",
      "Epoch [1][600/2217]\tLoss: 4.417\n",
      "Epoch [1][700/2217]\tLoss: 4.417\n",
      "Epoch [1][800/2217]\tLoss: 4.414\n",
      "Epoch [1][900/2217]\tLoss: 4.413\n",
      "Epoch [1][1000/2217]\tLoss: 4.411\n",
      "Epoch [1][1100/2217]\tLoss: 4.412\n",
      "Epoch [1][1200/2217]\tLoss: 4.411\n",
      "Epoch [1][1300/2217]\tLoss: 4.411\n",
      "Epoch [1][1400/2217]\tLoss: 4.410\n",
      "Epoch [1][1500/2217]\tLoss: 4.409\n",
      "Epoch [1][1600/2217]\tLoss: 4.409\n",
      "Epoch [1][1700/2217]\tLoss: 4.408\n",
      "Epoch [1][1800/2217]\tLoss: 4.407\n",
      "Epoch [1][1900/2217]\tLoss: 4.407\n",
      "Epoch [1][2000/2217]\tLoss: 4.406\n",
      "Epoch [1][2100/2217]\tLoss: 4.405\n",
      "Epoch [1][2200/2217]\tLoss: 4.404\n",
      "Epoch [2][0/2217]\tLoss: 4.325\n",
      "Epoch [2][100/2217]\tLoss: 4.315\n",
      "Epoch [2][200/2217]\tLoss: 4.302\n",
      "Epoch [2][300/2217]\tLoss: 4.308\n",
      "Epoch [2][400/2217]\tLoss: 4.308\n",
      "Epoch [2][500/2217]\tLoss: 4.310\n",
      "Epoch [2][600/2217]\tLoss: 4.308\n",
      "Epoch [2][700/2217]\tLoss: 4.309\n",
      "Epoch [2][800/2217]\tLoss: 4.310\n",
      "Epoch [2][900/2217]\tLoss: 4.312\n",
      "Epoch [2][1000/2217]\tLoss: 4.312\n",
      "Epoch [2][1100/2217]\tLoss: 4.313\n",
      "Epoch [2][1200/2217]\tLoss: 4.312\n",
      "Epoch [2][1300/2217]\tLoss: 4.310\n",
      "Epoch [2][1400/2217]\tLoss: 4.309\n",
      "Epoch [2][1500/2217]\tLoss: 4.308\n",
      "Epoch [2][1600/2217]\tLoss: 4.307\n",
      "Epoch [2][1700/2217]\tLoss: 4.307\n",
      "Epoch [2][1800/2217]\tLoss: 4.305\n",
      "Epoch [2][1900/2217]\tLoss: 4.304\n",
      "Epoch [2][2000/2217]\tLoss: 4.303\n",
      "Epoch [2][2100/2217]\tLoss: 4.302\n",
      "Epoch [2][2200/2217]\tLoss: 4.302\n",
      "Epoch [3][0/2217]\tLoss: 4.067\n",
      "Epoch [3][100/2217]\tLoss: 4.185\n",
      "Epoch [3][200/2217]\tLoss: 4.186\n",
      "Epoch [3][300/2217]\tLoss: 4.194\n",
      "Epoch [3][400/2217]\tLoss: 4.195\n",
      "Epoch [3][500/2217]\tLoss: 4.199\n",
      "Epoch [3][600/2217]\tLoss: 4.202\n",
      "Epoch [3][700/2217]\tLoss: 4.202\n",
      "Epoch [3][800/2217]\tLoss: 4.202\n",
      "Epoch [3][900/2217]\tLoss: 4.203\n",
      "Epoch [3][1000/2217]\tLoss: 4.204\n",
      "Epoch [3][1100/2217]\tLoss: 4.205\n",
      "Epoch [3][1200/2217]\tLoss: 4.207\n",
      "Epoch [3][1300/2217]\tLoss: 4.209\n",
      "Epoch [3][1400/2217]\tLoss: 4.210\n",
      "Epoch [3][1500/2217]\tLoss: 4.209\n",
      "Epoch [3][1600/2217]\tLoss: 4.208\n",
      "Epoch [3][1700/2217]\tLoss: 4.208\n",
      "Epoch [3][1800/2217]\tLoss: 4.206\n",
      "Epoch [3][1900/2217]\tLoss: 4.206\n",
      "Epoch [3][2000/2217]\tLoss: 4.207\n",
      "Epoch [3][2100/2217]\tLoss: 4.206\n",
      "Epoch [3][2200/2217]\tLoss: 4.206\n",
      "Epoch [4][0/2217]\tLoss: 4.074\n",
      "Epoch [4][100/2217]\tLoss: 4.108\n",
      "Epoch [4][200/2217]\tLoss: 4.109\n",
      "Epoch [4][300/2217]\tLoss: 4.113\n",
      "Epoch [4][400/2217]\tLoss: 4.114\n",
      "Epoch [4][500/2217]\tLoss: 4.114\n",
      "Epoch [4][600/2217]\tLoss: 4.113\n",
      "Epoch [4][700/2217]\tLoss: 4.115\n",
      "Epoch [4][800/2217]\tLoss: 4.118\n",
      "Epoch [4][900/2217]\tLoss: 4.118\n",
      "Epoch [4][1000/2217]\tLoss: 4.120\n",
      "Epoch [4][1100/2217]\tLoss: 4.122\n",
      "Epoch [4][1200/2217]\tLoss: 4.124\n",
      "Epoch [4][1300/2217]\tLoss: 4.125\n",
      "Epoch [4][1400/2217]\tLoss: 4.126\n",
      "Epoch [4][1500/2217]\tLoss: 4.128\n",
      "Epoch [4][1600/2217]\tLoss: 4.128\n",
      "Epoch [4][1700/2217]\tLoss: 4.129\n",
      "Epoch [4][1800/2217]\tLoss: 4.130\n",
      "Epoch [4][1900/2217]\tLoss: 4.131\n",
      "Epoch [4][2000/2217]\tLoss: 4.132\n",
      "Epoch [4][2100/2217]\tLoss: 4.133\n",
      "Epoch [4][2200/2217]\tLoss: 4.134\n"
     ]
    }
   ],
   "source": [
    "# execute training\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "    torch.save(state, 'data/checkpoint_' + str(epoch) + '.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ceba9397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  how do you feel ? \n",
      "Maximum Reply Length:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont know\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  how is the weather today ? \n",
      "Maximum Reply Length:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont know\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  quit\n"
     ]
    }
   ],
   "source": [
    "# testing / application of the model in chat\n",
    "checkpoint = torch.load('data/checkpoint_4.pth.tar')\n",
    "transformer = checkpoint['transformer']\n",
    "\n",
    "while(1):\n",
    "    question = input(\"Question: \") \n",
    "    if question == 'quit':\n",
    "        break\n",
    "    max_len = input(\"Maximum Reply Length: \")\n",
    "    enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a0f7d",
   "metadata": {},
   "source": [
    "# TODO's\n",
    "\n",
    "1. Send your finished presentations (+ possibly annotated paper) by **Monday 12.00 AM/midnight** via email to henrik.voigt@uni-jena.de\n",
    "\n",
    "2. Send your little HOMEWORK to henrik.voigt@uni-jena.de by using the naming convention: HOMEWORK_02_FIRSTNAME_LASTNAME.ipynb until **June 23th 12.00 AM/midnight**\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env-kernel",
   "language": "python",
   "name": "pytorch-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
